var relearn_searchindex = [
  {
    "breadcrumb": "Transactional Event Queues \u003e Migrating From AQ",
    "content": "This section provides a detailed guide for migrating from Oracle Advanced Queuing (AQ) to Transactional Event Queues (TxEventQ). The migration process uses the DBMS_AQMIGTOOL package to ensure minimal disruption of existing messaging workflows.\nUsers of AQ are recommended to migrate to TxEventQ for increased support, performance, and access to new database features. It is recommended to read through the document fully before attempting migration.\nDBMS_AQMIGTOOL Overview Migration Workflow Checking Compatibility Initiating Migration Checking Migration Status Commit the Migration Checking and Handling Migration Errors Cancelling and Recovering Migration Limitations and Workarounds DBMS_AQMIGTOOL Overview The migration tool interface provides the following functionalities:\nMigration may be performed in AUTOMATIC, INTERACTIVE, OFFLINE, or ONLY_DEFINITION mode, each mode providing specific use cases and benefits. DBMS_AQMIGTOOL.AUTOMATIC: Enqueue and dequeue operations are allowed during migration, and a background job will commit the migration once the AQ is empty and no unsupported features are detected. DBMS_AQMIGTOOL.INTERACTIVE (Default): Enqueue and dequeue operations are allowed, and the user must commit the migration. DBMS_AQMIGTOOL.OFFLINE: Only dequeue operations are allowed during migration, which can help in draining the AQ. DBMS_AQMIGTOOL.ONLY_DEFINITION: A TxEventQ copy is made from the AQ configuration, and messages are not migrated. Both the AQ and TxEventQ remain in the system after migration is committed with separate message streams. This option is recommended for users who prefer a more manual or custom migration. Users may decide to commit the migration or revert to AQ via the migration interface. During migration, in-flight messages can be tracked by viewing messages not in the PROCESSED state for both the AQ and TxEventQ. A migration history is recorded for all queues. Users may optionally purge old AQ messages if they wish to discard the data after migration. AQ Migration supports both rolling upgrades and Oracle GoldenGate (OGG) replication. During online migrations, it is safe for applications to continue enqueue/dequeue operations as normal. Migration Workflow Check compatibility of migration and review the limitations and workarounds.\nStart the migration with the DBMS_AQMIGTOOL.INIT_MIGRATION procedure, which creates an interim TxEventQ by copying the AQ’s configuration, including payload type, rules, subscribers, privileges, notifications, and more. Administrative queue changes (alter queue) are restricted until the migration is completed or canceled.\nDuring migration, queue messages transition to the TxEventQ. New messages go to TxEventQ, and dequeue requests go to AQ until it is drained of messages at which point dequeue requests switch to TxEventQ. Migration status can be monitored and any errors addressed or rolled back.\nOnce the AQ is drained or purged, the DBMS_AQMIGTOOL.COMMIT_MIGRATION procedure drops the AQ and renames the interim TxEventQ to the AQ’s name, ensuring application compatibility.\nIf using DBMS_AQMIGTOOL.AUTOMATIC migration mode, commit is not required. In the event of an error that prevents migration from continuing, AQ migration may be cancelled and the in-flight messages restored to the AQ without data loss.\nChecking Compatibility The CHECK_MIGRATION_TO_TXEVENTQ procedure is used to verify an AQ is suitable for migration to TxEventQ and should be run before any migration is attempted.\nThe following SQL script creates a migration report for the AQ my_queue and prints the count of events in the migration report. If all features are supported, the migration_report will be empty.\ndeclare migration_report sys.TxEventQ_MigReport_Array := sys.TxEventQ_MigReport_Array(); begin DBMS_AQMIGTOOL.CHECK_MIGRATION_TO_TXEVENTQ('testuser', 'my_queue', migration_report); dbms_output.put_line('Migration report unsupported events count: ' || migration_report.COUNT); end; / If the output is similar to “Migration report unsupported events count: 0”, then it is safe to initiate the migration.\nInitiating Migration The INIT_MIGRATION procedure is used to begin AQ migration to TxEventQ. The mig_mode parameter is used to control the migration type and defaults to DBMS_AQMIGTOOL.INTERACTIVE which allows both enqueue and dequeue during migration. A full description of mig_mode options is available in the procedure documentation.\nThe following SQL script starts migration for the AQ my_queue in the testuser schema. Once migration starts, an interim TxEventQ will be created named my_queue_m that includes a copy of the existing AQ configuration.\nbegin DBMS_AQMIGTOOL.INIT_MIGRATION( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue' ); end; / INIT_MIGRATION is non-blocking may be executed concurrently on multiple queues. If DBMS_AQMIGTOOL.INTERACTIVE is used as the migration mode, messages may be enqueued and dequeued during migration.\nChecking Migration Status While migration is occurring, dequeue will pull from the AQ until it is empty, at which point it will switch to the TxEventQ.\nThe following SQL statement uses the CHECK_MIGRATED_MESSAGES procedure to print the count of remaining READY state messages in the AQ and the count messages migrated to TxEventQ. Once the aq_msg_cnt reaches zero, the AQ has been fully drained and the migration can be committed. If we attempt to commit the migration before the AQ is drained of READY state messages, an exception will be raised.\ndeclare migrated_q_msg_cnt number := 0; aq_msg_cnt number := 0; begin DBMS_AQMIGTOOL.CHECK_MIGRATED_MESSAGES( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue', txeventq_migrated_message =\u003e migrated_q_msg_cnt, cq_pending_messages =\u003e aq_msg_cnt ); dbms_output.put_line('AQ ready state message count: ' || aq_msg_cnt); dbms_output.put_line('Migrated TxEventQ message count: ' || migrated_q_msg_cnt); end; / The USER_TXEVENTQ_MIGRATION_STATUS view is used to check the status of in-process migrations for all queues owned by the current user. Queue information, migration status, and error events are available in this view.\nCommit the Migration Once the count of READY state messages in the AQ reach zero, the COMMIT_MIGRATION procedure is used to finalize the migration operation.\nThe following SQL script commits the migration for the my_queue queue in the testuser schema. Once the migration is committed, my_queue becomes a TxEventQ.\nbegin DBMS_AQMIGTOOL.COMMIT_MIGRATION( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue' ); end; / If you wish to purge all messages from the AQ rather than dequeuing them, the PURGE_QUEUE_MESSAGES procedure can be used to empty the queue.\nThe following SQL script purges messages from the my_queue queue in the testuser schema, allowing migration commit without dequeuing the AQ.\nbegin DBMS_AQMIGTOOL.PURGE_QUEUE_MESSAGES( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue' ); end; Checking and Handling Migration Errors If you encounter an error during AQ migration or are using an AQ feature that is unsupported in TxEventQ, the CHECK_STATUS procedure can be used to query the current migration status and report any errors.\nThe following SQL script checks the migration status of the my_queue queue. In the event of migration incompatibilities, application changes may be required before continuing migration.\ndeclare mig_STATUS VARCHAR2(128); mig_comments VARCHAR2(1024); begin DBMS_AQMIGTOOL.CHECK_STATUS( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue', status =\u003e mig_STATUS, migration_comment =\u003e mig_comments ); dbms_output.put_line('Migration Status: ' || mig_STATUS); dbms_output.put_line('Migration comments: ' || mig_comments); end; / Example of an error output due to unsupported TxEventQ enqueue operations:\nMigration Status: Compatibility Error: Transformation in Enq Unsupported Feature Migration comments: Unsupported parameter in Enqueue Cancelling and Recovering Migration The CANCEL_MIGRATION procedure is used to cancel an in-progress migration. Use DBMS_AQMIGTOOL.RESTORE as the cancellation mode (default) to preserve messages during cancellation.\nThe following SQL script cancels migration for the AQ my_queue in the testuser schema:\nbegin DBMS_AQMIGTOOL.CANCEL_MIGRATION( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue', cancelmode =\u003e DBMS_AQMIGTOOL.RESTORE ); end; / The RECOVER_MIGRATION procedure can be used to restore a migration to the nearest safe point, either before or after the execution of DBMS_AQMIGTOOL.CANCEL_MIGRATION, DBMS_AQMIGTOOL.COMMIT_MIGRATION, or DBMS_AQMIGTOOL.INIT_MIGRATION. You may wish to use the RECOVER_MIGRATION procedure if an unexpected error has occurred, or the use of an unsupported feature is detected in the USER_TXEVENTQ_MIGRATION_STATUS view.\nThe following SQL script recovers migration to the nearest safe point, and outputs the recovery message:\ndeclare recovery_message VARCHAR2(1024); begin DBMS_AQMIGTOOL.RECOVER_MIGRATION( cqschema =\u003e 'testuser', cqname =\u003e 'my_queue', recovery_message =\u003e recovery_message ); dbms_output.put_line('Recovery message: ' || recovery_message); end; / Limitations and Workarounds The following features are unsupported in TxEventQ, and should be mitigated before migration:\nQueue Retry Delay: Set retry_delay to zero using DBMS_AQADM.ALTER_QUEUE. Message transaction on enqueue/dequeue: Move the transformation to the application layer. Multi-queue listeners: Implement single queue listening with dequeue browse. Priority values outside the range 0-9: Adjust priority values to the range 0-9. The following features do not support migration. Applications using these features must be modified before migration:\nMessage grouping. Sequence deviation and relative message id. Message recipient lists.",
    "description": "This section provides a detailed guide for migrating from Oracle Advanced Queuing (AQ) to Transactional Event Queues (TxEventQ). The migration process uses the DBMS_AQMIGTOOL package to ensure minimal disruption of existing messaging workflows.\nUsers of AQ are recommended to migrate to TxEventQ for increased support, performance, and access to new database features. It is recommended to read through the document fully before attempting migration.\nDBMS_AQMIGTOOL Overview Migration Workflow Checking Compatibility Initiating Migration Checking Migration Status Commit the Migration Checking and Handling Migration Errors Cancelling and Recovering Migration Limitations and Workarounds DBMS_AQMIGTOOL Overview The migration tool interface provides the following functionalities:",
    "tags": [],
    "title": "AQ Migration",
    "uri": "/microservices-datadriven/transactional-event-queues/aq-migration/migration/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Getting Started",
    "content": "This section provides the basic concepts of Transactional Event Queues, including the difference between queues and topics, how to create queues using SQL, and the available message payload types.\nWhat are queues and topics? Message Payload Types DBMS_AQADM.JMS_TYPE Raw JSON Object Kafka Message Payloads What are queues and topics? Queues and topics both provide high-throughput, asynchronous application communication, but have a few key differences that are relevant for developers and architects.\nWhen using queues, messages follow a send/receive model that allows exactly one consumer. Once the message is consumed, it is discarded after a configurable interval. In contrast, topics may have multiple consumers for each message, and the message is persisted for as long as specified.\nQueues work best for applications that expect to dequeue messages to a single consumer. If you plan to broadcast messages to multiple consumers, or require message replay, you should use topics. In general, topics provide greater flexibility for event-streaming applications due to the ability of consumers to independently consume or replay messages, and the database’s capability of persisting the message for a specified duration.\nThe following SQL script uses the DBMS_AQADM.CREATE_TRANSACTIONAL_EVENT_QUEUE procedure to create a topic by setting the multiple_consumers parameter to true. You can find the full parameter definition for the DBMS_AQADM.CREATE_TRANSACTIONAL_EVENT_QUEUE procedure here.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'my_queue', queue_payload_type =\u003e DBMS_AQADM.JMS_TYPE, multiple_consumers =\u003e true ); -- Start the queue dbms_aqadm.start_queue( queue_name =\u003e 'my_queue' ); end; / Message Payload Types Transactional Event Queues support various message payload types, allowing flexibility in how data is structured and stored within the queue. The payload type determines the format and structure of the messages that can be enqueued and dequeued. When creating a Transactional Event Queue, you may specify one of these payload types, each offering different benefits depending on your application’s needs and data characteristics. Note that if not specified, DBMS_AQADM.JMS_TYPE is the default payload type.\nUnderstanding these payload types is crucial for designing efficient and effective messaging solutions. The choice of payload type impacts how you interact with the queue, the kind of data you can store, and how that data is processed. Let’s explore the available payload types and their use cases.\nAttempting to produce a message of the wrong payload type may result in the following error:\nORA-25207: enqueue failed, queue . is disabled from enqueueing\nDBMS_AQADM.JMS_TYPE The JMS (Java Message Service) payload type is ideal for applications using JMS, as it provides a highly scalable API for asynchronous messaging.\nThe following script creates and starts a Transactional Event Queue using DBMS_AQADM.JMS_TYPE as the payload type, which is the default payload type.\n-- Create a Transactional Event Queue begin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'my_queue', -- Payload can be RAW, JSON, DBMS_AQADM.JMS_TYPE, or an object type. -- Default is DBMS_AQADM.JMS_TYPE. queue_payload_type =\u003e DBMS_AQADM.JMS_TYPE, multiple_consumers =\u003e false ); -- Start the queue dbms_aqadm.start_queue( queue_name =\u003e 'my_queue' ); end; / Raw When using the RAW type, the Transactional Event Queue backing table will be created with a Large Object (LOB) column 32k in size for binary messages.\nRAW payloads are suitable for unstructured binary data that does not fit into predefined schemas, or for simple, lightweight messages. While RAW payloads offer flexibility and efficiency, they may require additional application level processing to interpret the binary data.\nThe following SQL script creates a Transactional Event Queue using the RAW payload type.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'json_queue', queue_payload_type =\u003e 'RAW' ); dbms_aqadm.start_queue( queue_name =\u003e 'json_queue' ); end; / JSON The JSON payload type stores the JSON message data in a post-parse binary format, allowing fast access to nested JSON values. It’s recommended to use the JSON payload type if you’re working with document data or other unstructured JSON.\nThe following SQL script creates a Transactional Event Queue using the JSON payload type.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'json_queue', queue_payload_type =\u003e 'JSON' ); dbms_aqadm.start_queue( queue_name =\u003e 'json_queue' ); end; / Object For structured, complex messages, you may choose to set the payload type as a custom object type that was defined using create type. Object types must be accessible from the queue/topic, and the structure of each message must exactly match the payload type.\nThe following SQL script defines a custom object type, and then creates a Transactional Event Queue using that type.\n-- Define the payload type create type my_message as object ( id number, subject varchar2(100), body varchar2(2000) ); -- Create and start a queue using the custom payload type begin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'custom_type_queue', queue_payload_type =\u003e 'my_message' ); -- Start the queue dbms_aqadm.start_queue( queue_name =\u003e 'custom_type_queue' ); end; / Kafka Message Payloads Topics created using the Kafka APIs for Transactional Event Queues use a Kafka message payload type, and specifying the payload type is not necessary. Additionally, topics created using Kafka APIs should also be managed and interacted with using the appropriate Kafka APIs.\nFor more information on using Kafka APIs with TxEventQ, see the Kafka chapter.",
    "description": "This section provides the basic concepts of Transactional Event Queues, including the difference between queues and topics, how to create queues using SQL, and the available message payload types.\nWhat are queues and topics? Message Payload Types DBMS_AQADM.JMS_TYPE Raw JSON Object Kafka Message Payloads What are queues and topics? Queues and topics both provide high-throughput, asynchronous application communication, but have a few key differences that are relevant for developers and architects.",
    "tags": [],
    "title": "Core Concepts",
    "uri": "/microservices-datadriven/transactional-event-queues/getting-started/core-concepts/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "Oracle Database Transactional Event Queues (TxEventQ) is a high-performance messaging platform built into Oracle Database, designed for application workflows, microservices, and event-driven architectures. This guide will provide you with a thorough understanding of Oracle Database Transactional Event Queues, enabling you to leverage its powerful features for building robust, scalable, and event-driven applications.\nThis module will cover the following key topics:\nCore Concepts Queues and topics Enqueue/Dequeue vs. Publish/Subscribe models Payload types: RAW, Abstract Data Type (ADT), JSON, and JMS Queue Management Necessary grants, roles, and permissions for using queues Creating, starting, stopping, and dropping queues/topics in various languages SQL examples for queue operations Message Operations Producing and consuming messages Message expiration and exception queues Message Delay Message Priority Transactional messaging: Combining messaging and DML in a single transaction Advanced Features Message propagation between queues and databases Exception queues and error handling",
    "description": "Oracle Database Transactional Event Queues (TxEventQ) is a high-performance messaging platform built into Oracle Database, designed for application workflows, microservices, and event-driven architectures. This guide will provide you with a thorough understanding of Oracle Database Transactional Event Queues, enabling you to leverage its powerful features for building robust, scalable, and event-driven applications.\nThis module will cover the following key topics:\nCore Concepts Queues and topics Enqueue/Dequeue vs. Publish/Subscribe models Payload types: RAW, Abstract Data Type (ADT), JSON, and JMS Queue Management Necessary grants, roles, and permissions for using queues Creating, starting, stopping, and dropping queues/topics in various languages SQL examples for queue operations Message Operations Producing and consuming messages Message expiration and exception queues Message Delay Message Priority Transactional messaging: Combining messaging and DML in a single transaction Advanced Features Message propagation between queues and databases Exception queues and error handling",
    "tags": [],
    "title": "Getting Started",
    "uri": "/microservices-datadriven/transactional-event-queues/getting-started/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Kafka APIs",
    "content": "This section describes the architectural concepts of Oracle Database Transactional Event Queues in the context of Apache Kafka APIs. When working with Kafka APIs, we’ll refer to queues as topics.\nKafka Brokers or a Database Server? Topics, Producers, and Consumers Partitions and Ordering Partition Keys Message Offsets Message Commits Kafka Brokers or a Database Server? When using Oracle Database Transactional Event Queues with Kafka APIs, the database server assumes the role of the Kafka Broker, eliminating the need for additional messaging systems.\nUsing your database as a message broker allows you to avoid separate, costly servers dedicated to event streaming. These servers typically require domain-specific knowledge to operate, maintain, and upgrade in production deployments.\nWith a database message broker, your messaging data is co-located with your other data and remains queryable with SQL. This reduces network traffic and data duplication across multiple servers (and their associated costs), while benefiting applications that need access to both event streaming data and its table-based data.\nTopics, Producers, and Consumers A topic is a logical channel for message streams, capable of high-throughput messaging. Producers write data to topics, producing messages. Consumers subscribe to topics and poll message data. Each consumer is part of a consumer group, which is a logical grouping of consumers, their subscriptions, and assignments.\nWith Oracle Database Transactional Event Queues, each topic is backed by a queue table, allowing transactional messaging and query capabilities. For example, you can query five messages from a topic named my_topic directly with SQL:\nselect * from my_topic fetch first 5 rows only; When using Kafka APIs for Transactional Event Queues, you may also run database queries as part of consumer and producer workflows.\nPartitions and Ordering Topics are divided into one or more partitions, where each partition is backed by a Transactional Event Queue event stream. A partition represents an ordered event stream within the topic.\nPartitions enable parallel message consumption, as multiple consumers in the same consumer group can concurrently poll from the topic. Consumers are assigned one or more partitions depending on the size of the consumer group. Each partition, however, may be assigned to at most one consumer per group. For example, a topic with three partitions can have at most three active consumers per consumer group.\nWithin a partition, each message is strictly ordered. If each consumer in a consumer group is assigned to only one partition, then the consumers in the group are guaranteed to receive only the ordered stream of messages from their assigned partition. Additionally, this strategy will commonly result in the highest throughput per topic.\nPartition Keys Partition keys are used to route messages to specific partitions, ensuring the ordering of messages is preserved in the partition. To ensure strict ordering, use a consistent piece of message data as the partition key for each producer record. For example, if you are processing user interactions, the use of the user ID as a partition key will ensure each user’s messages are produced to a consistent partition. The user ID is implicitly hashed to produce a consistent partition key.\nThe use of partition keys is not mandatory: round-robin partition assignment is used if no partition key is provided in the producer record. Round-robin assignment is convenient when there is no specific ordering of messages, or strict ordering is not required.\nNote that you may directly specify the topic partition when producing a message, instead of relying on key hashing. This is useful when there is no message data suitable for the partition key, or if you require a direct level of control over partitioning.\nMessage Offsets Offsets are used to track the progress of consumers as they consume messages from topics. An offset is an identifier for a specific position in a topic’s partition. When a consumer reads a message, it commits the offset to indicate that the message is being processed. Offsets serve as consumer checkpoints and can be used to replay messages. Replay is useful for debugging, reprocessing failed events, or back-filling data.\nFor example, consumers can be started from the earliest known offset and progress to the newest message, started at the latest known offset, or have their offset moved to a specific point in the partition.\nMessage Commits With Oracle Database Transactional Event Queues, each message operation occurs within a database transaction. For example, a producer record is not present in the database until the producer commits the current transaction. For consumers, their offset is not recorded until the consumer commits the current transaction.\nUsing the Kafka Java Client for Oracle Database Transactional Event Queues, producers and consumers can directly manage transaction-level details, including commit and abort.",
    "description": "This section describes the architectural concepts of Oracle Database Transactional Event Queues in the context of Apache Kafka APIs. When working with Kafka APIs, we’ll refer to queues as topics.\nKafka Brokers or a Database Server? Topics, Producers, and Consumers Partitions and Ordering Partition Keys Message Offsets Message Commits Kafka Brokers or a Database Server? When using Oracle Database Transactional Event Queues with Kafka APIs, the database server assumes the role of the Kafka Broker, eliminating the need for additional messaging systems.",
    "tags": [],
    "title": "Kafka and TxEventQ Concepts",
    "uri": "/microservices-datadriven/transactional-event-queues/kafka/concepts/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Spring Boot Integration",
    "content": "Java Message Service (JMS) is an API that provides a standardized way for Java applications to create, send, receive, and read messages in messaging systems. Spring JMS uses idiomatic APIs to produce and consume messages with JMS, using the JMSTemplate class and @JMSListener annotations for producers and consumers, respectively.\nIn this section, we’ll implement a producer/consumer example using Spring JMS with Oracle Database Transactional Event Queues JMS APIs.\nProject Dependencies Configure Permissions and Create a JMS Queue Connect Spring JMS to Oracle Database JMSTemplate Producer Receive messages with @JMSListener Project Dependencies To start developing with Spring JMS for Oracle Database Transactional Event Queues, add the oracle-spring-boot-starter-aqjms dependency to your Maven project, along with the Spring Boot JDBC starter:\n\u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-jdbc\u003c/artifactId\u003e \u003cversion\u003e${spring-boot.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.oracle.database.spring\u003c/groupId\u003e \u003cartifactId\u003eoracle-spring-boot-starter-aqjms\u003c/artifactId\u003e \u003cversion\u003e${oracle-database-starters.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle users, add the following dependency to your project:\nimplementation \"org.springframework.boot:spring-boot-starter-data-jdbc:${springBootVersion}\" implementation \"com.oracle.database.spring:oracle-spring-boot-starter-aqjms:${oracleDatabaseStartersVersion}\" Configure Permissions and Create a JMS Queue The following SQL script grants the necessary permissions to a database user for using Transactional Event Queues with JMS and creates a Transactional Event Queue with a JMS Payload Type for the user:\ngrant aq_user_role to testuser; grant execute on dbms_aq to testuser; grant execute on dbms_aqadm to testuser; grant execute on dbms_aqin to testuser; grant execute on dbms_aqjms to testuser; grant execute on dbms_teqk to testuser; begin -- Create a JMS queue dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'testuser.testqueue', queue_payload_type =\u003e DBMS_AQADM.JMS_TYPE, -- FALSE means queues can only have one consumer for each message. This is the default. -- TRUE means queues created in the table can have multiple consumers for each message. multiple_consumers =\u003e false ); -- Start the queue dbms_aqadm.start_queue( queue_name =\u003e 'testuser.testqueue' ); end; / Connect Spring JMS to Oracle Database Spring JMS with Oracle Database Transactional Event Queues uses a standard Oracle Database JDBC connection for message production and consumption. To configure this with YAML-style Spring datasource properties, it’ll look something like this (src/main/resources/application.yaml):\nspring: datasource: username: ${USERNAME} password: ${PASSWORD} url: ${JDBC_URL} driver-class-name: oracle.jdbc.OracleDriver type: oracle.ucp.jdbc.PoolDataSourceImpl oracleucp: initial-pool-size: 1 min-pool-size: 1 max-pool-size: 30 connection-pool-name: UCPSampleApplication connection-factory-class-name: oracle.jdbc.pool.OracleDataSource Additionally, you should define a JMS ConnectionFactory bean using the AQjmsFactory class. The ConnectionFactory bean ensures Spring JMS uses Oracle Database Transactional Event Queues as the JMS provider for message operations.\n@Bean public ConnectionFactory aqJmsConnectionFactory(DataSource ds) throws JMSException { return AQjmsFactory.getConnectionFactory(ds); } JMSTemplate Producer The Spring JMSTemplate bean provides an interface for JMS operations, including producing and consuming messages. The following class uses the jmsTemplate.convertAndSend() method to produce a message to a queue.\nimport org.springframework.beans.factory.annotation.Value; import org.springframework.jms.core.JmsTemplate; import org.springframework.stereotype.Component; @Component public class Producer { private final JmsTemplate jmsTemplate; private final String queueName; public Producer(JmsTemplate jmsTemplate, @Value(\"${txeventq.queue.name:testqueue}\") String queueName) { this.jmsTemplate = jmsTemplate; this.queueName = queueName; } public void enqueue(String message) { jmsTemplate.convertAndSend(queueName, message); } } Receive messages with @JMSListener The @JMSListener annotation may be applied to a method to configure a receiver for messages from a JMS queue.\nThe following class uses the @JMSListener annotation to read messages from a queue named testqueue, printing each message to stdout.\nimport org.springframework.beans.factory.annotation.Value; import org.springframework.jms.annotation.JmsListener; import org.springframework.stereotype.Component; @Component public class Consumer { @JmsListener(destination = \"${txeventq.queue.name:testqueue}\", id = \"sampleConsumer\") public void receiveMessage(String message) { System.out.printf(\"Received message: %s%n\", message); } }",
    "description": "Java Message Service (JMS) is an API that provides a standardized way for Java applications to create, send, receive, and read messages in messaging systems. Spring JMS uses idiomatic APIs to produce and consume messages with JMS, using the JMSTemplate class and @JMSListener annotations for producers and consumers, respectively.\nIn this section, we’ll implement a producer/consumer example using Spring JMS with Oracle Database Transactional Event Queues JMS APIs.\nProject Dependencies Configure Permissions and Create a JMS Queue Connect Spring JMS to Oracle Database JMSTemplate Producer Receive messages with @JMSListener Project Dependencies To start developing with Spring JMS for Oracle Database Transactional Event Queues, add the oracle-spring-boot-starter-aqjms dependency to your Maven project, along with the Spring Boot JDBC starter:",
    "tags": [],
    "title": "Spring JMS",
    "uri": "/microservices-datadriven/transactional-event-queues/spring-boot/jms/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Performance and Observability",
    "content": "Oracle Database provides administrative views for collecting metrics on TxEventQ topics and queues. In this section, you’ll learn about each view and its use by database administrators for queue performance monitoring.\nTxEventQ Views Find the list of TxEventQ administrative views and their column definitions in the Oracle Database TxEventQ documentation. Views may be joined and grouped across queries to compose custom metrics and insights about queues and subscribers.\nV$EQ_CACHED_PARTITIONS Provides information about cached event stream partitions. Queries may group on identifiers like QUEUE id or EVENT_STREAM_ID.\nV$EQ_CROSS_INSTANCE_JOBS The V$EQ_CROSS_INSTANCE_JOBS view provides information about TxEventQ cross-instance jobs. This view is crucial for monitoring and managing message forwarding across different instances in a database cluster. The view offers comprehensive data about each job, including:\nJob ID Source schema and queue name Event stream details Destination instance Job state and performance metrics V$EQ_DEQUEUE_SESSIONS The V$EQ_DEQUEUE_SESSIONS view provides information about active dequeue sessions for Transactional Event Queues. The view displays real-time data about sessions that are currently dequeuing messages. Queries may group on fields like QUEUE_ID, SUBSCRIBER_ID, CLIENT_ID, and more.\nV$EQ_INACTIVE_PARTITIONS The V$EQ_INACTIVE_PARTITIONS view provides information about inactive TxEventQ event stream partitions. The view is useful for identifying inactive partitions by their QUEUE_NAME or EVENT_STREAM_ID.\nV$EQ_MESSAGE_CACHE The V$EQ_MESSAGE_CACHE view provides performance statistics for the message cache associated with queues at the event stream partition level within an instance. This view is particularly useful for monitoring and diagnosing the behavior of event stream partitions in terms of message handling and memory usage.\nV$EQ_MESSAGE_CACHE_ADVICE The V$EQ_MESSAGE_CACHE_ADVICE view provides simulation metrics to assist in sizing the message cache for queues. By analyzing various potential cache sizes, this view helps determine the optimal configuration to ensure efficient message handling and system performance.\nV$EQ_MESSAGE_CACHE_STAT The V$EQ_MESSAGE_CACHE_STAT view provides global statistics on memory management for queues in the Streams pool of the System Global Area (SGA). This view offers insights into the behavior and performance of event queue partitions across all TxEventQs, aiding in effective monitoring and optimization. The view shows statistics across all queues.\nV$EQ_NONDUR_SUBSCRIBER The V$EQ_NONDUR_SUBSCRIBER view provides details about non-durable subscriptions on queues. Non-durable subscribers receive messages only while actively connected; they do not retain message state after disconnection. The view is per-queue and non-durable subscriber.\nV$EQ_NONDUR_SUBSCRIBER_LWM The V$EQ_NONDUR_SUBSCRIBER_LWM view provides information about the low watermarks (LWMs) of non-durable subscribers in a Transactional Event Queue (TxEventQ). The LWM of a subscriber represents a position within an event stream below which none of the messages are of interest to the subscriber.\nV$EQ_PARTITION_STATS The V$EQ_PARTITION_STATS view provides usage statistics for queue partition caches, specifically focusing on the queue partition cache and the dequeue log partition cache. This view is instrumental in monitoring and optimizing the performance of these caches.\nV$EQ_REMOTE_DEQUEUE_AFFINITY The V$EQ_REMOTE_DEQUEUE_AFFINITY view provides information about subscribers who are dequeuing messages from queues on an instance different from the event stream’s owner instance. In such cases, cross-instance message forwarding is employed to deliver messages to these subscribers.\nThis view is useful for monitoring and managing cross-instance message forwarding in a Real Application Clusters (RAC) environment, ensuring that subscribers receive messages even when dequeuing from a different instance than the event stream’s owner.\nV$EQ_SUBSCRIBER_LOAD The V$EQ_SUBSCRIBER_LOAD view provides data on the load and latency of all subscribers to queue event streams across instances in an Oracle Real Application Clusters (RAC) environment. This view helps in monitoring subscriber performance and identifying potential bottlenecks.\nV$EQ_SUBSCRIBER_STAT The V$EQ_SUBSCRIBER_STAT view provides statistics about subscribers of queue event streams. Each row corresponds to a specific combination of queue, event stream, and subscriber.\nThis view is useful for monitoring the performance and status of subscribers, allowing administrators to identify potential bottlenecks and optimize message processing.\nV$EQ_UNCACHED_PARTITIONS The V$EQ_UNCACHED_PARTITIONS view provides information about uncached partitions of queue event streams. Each row represents a specific event stream partition that is not currently cached.",
    "description": "Oracle Database provides administrative views for collecting metrics on TxEventQ topics and queues. In this section, you’ll learn about each view and its use by database administrators for queue performance monitoring.\nTxEventQ Views Find the list of TxEventQ administrative views and their column definitions in the Oracle Database TxEventQ documentation. Views may be joined and grouped across queries to compose custom metrics and insights about queues and subscribers.\nV$EQ_CACHED_PARTITIONS Provides information about cached event stream partitions. Queries may group on identifiers like QUEUE id or EVENT_STREAM_ID.",
    "tags": [],
    "title": "TxEventQ Administrative Views",
    "uri": "/microservices-datadriven/transactional-event-queues/observability/views/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Performance and Observability",
    "content": "The Oracle Database Metrics Exporter is a standalone application that provides observability into Oracle Database instances and is designed to run on-premises or in the cloud. This section covers configuring the database exporter for Oracle Database Transactional Event Queues.\nTo get started with the database exporter, see the installation section.\nExporting TxEventQ Metrics Sample Grafana Dashboard Exporting TxEventQ Metrics The database exporter supports custom metric definitions in the form of TOML files. The TxEventQ custom metrics file contains metrics definitions for queue information like total number of queues, enqueued messages, dequeued messages, and more. You can include the TxEventQ metrics in the database exporter by using the custom.metrics program argument and passing the file location:\n--custom.metrics=txeventq-metrics.toml Or, set the CUSTOM_METRICS environment variable to point to custom metric files:\nCUSTOM_METRICS=txeventq-metrics.toml To add more TxEventQ metrics, you can create a custom metrics file based on the views described in the TxEventQ Administrative Views section and then provide that file to the database exporter.\nSample Grafana Dashboard The Sample Dashboard for TxEventQ visualizes metrics related to database queuing, and requires the custom metrics definitions located in the TxEventQ custom metrics file to be loaded into the database exporter.\nThe dashboard can be loaded into a Grafana instance to visualize TxEventQ status, throughput, and subscriber information for a given queue.",
    "description": "The Oracle Database Metrics Exporter is a standalone application that provides observability into Oracle Database instances and is designed to run on-premises or in the cloud. This section covers configuring the database exporter for Oracle Database Transactional Event Queues.\nTo get started with the database exporter, see the installation section.\nExporting TxEventQ Metrics Sample Grafana Dashboard Exporting TxEventQ Metrics The database exporter supports custom metric definitions in the form of TOML files. The TxEventQ custom metrics file contains metrics definitions for queue information like total number of queues, enqueued messages, dequeued messages, and more. You can include the TxEventQ metrics in the database exporter by using the custom.metrics program argument and passing the file location:",
    "tags": [],
    "title": "Database Monitoring Exporter",
    "uri": "/microservices-datadriven/transactional-event-queues/observability/exporter/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Kafka APIs",
    "content": "This section provides developer-friendly examples using the Kafka Java Client for Oracle Database Transactional Event Queues. The Kafka Java client implements Kafka client interfaces, allowing you to use familiar Kafka Java APIs with Oracle Database Transactional Event Queues. You’ll learn how to authenticate to Oracle Database using SSL with Oracle Wallet or PLAINTEXT, create topics, produce messages, and consume messages using Java Kafka clients.\nKafka Java Client for Oracle Database Transactional Event Queues To get started using the client, add the dependency for the Kafka Java Client for Oracle Database Transactional Event Queues to your project. If you’re using Maven:\n\u003cdependency\u003e \u003cgroupId\u003ecom.oracle.database.messaging\u003c/groupId\u003e \u003cartifactId\u003eokafka\u003c/artifactId\u003e \u003cversion\u003e${okafka.version}\u003c/version\u003e \u003c/dependency\u003e Or, if you’re using Gradle:\nimplementation \"com.oracle.database.messaging:okafka:${okafkaVersion}\" The database user should have appropriate permissions to create topics, produce and consume messages. See the Kafka permissions section for recommended database permissions.\nAuthenticating to Oracle Database To authenticate to Oracle Database with the Kafka clients, configure a Java Properties object with Oracle Database-specific properties for service name, wallet location, and more.\nThe configured Properties objects are passed to Kafka Java Client for Oracle Database Transactional Event Queues implementations for Oracle Database authentication. We’ll use these authentication samples as a base for creating Kafka Java clients in follow-up examples.\nConfiguring Plaintext Authentication PLAINTEXT authentication uses an ojdbc.properties file to supply the database username and password to the Kafka Java client. Create a file named ojdbc.properties on your system, and populate it with your database username and password:\nuser = \u003cdatabase username\u003e password = \u003cdatabase password\u003e Next, in your Java application, create a Properties object and configure it with the following connection properties, as appropriate for your database:\nProperties props = new Properties(); // Database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Connection protocol. Set to either PLAINTEXT or SSL props.put(\"security.protocol\", \"PLAINTEXT\"); // Oracle Database hostname and (port) props.put(\"bootstrap.servers\", \"localhost:1521\"); // Path to the directory containing ojdbc.properties props.put(\"oracle.net.tns_admin\", \"\u003cojdbc.properties directory\u003e\"); Configuring SSL (Oracle Wallet) Authentication For connections authenticated using Oracle Database Wallet, use SSL as the security.protocol and provide the path to your unzipped Oracle Database Wallet using the oracle.net.tns_admin property. Note that the database wallet must be downloaded, unzipped, and readable by your Java application:\nProperties props = new Properties(); // Database service name props.put(\"oracle.service.name\", \"mypdb\"); // Connection protocol. Set to either PLAINTEXT or SSL props.put(\"security.protocol\", \"SSL\"); // Oracle Database hostname and (port) props.put(\"bootstrap.servers\", \"database_hostname\"); // Path to wallet directory props.put(\"oracle.net.tns_admin\", \"\u003cwallet directory\u003e\"); Creating Topics The org.oracle.okafka.clients.admin.AdminClient class implements the Kafka Java Client Admin interface, and should be used to create topics for Oracle Database Transactional Event Queues when using Kafka APIs.\nThe following Java class provides a sample implementation for topic creation. Assume the props parameter contains authenticating properties for Oracle Database, as defined in Authenticating to Oracle Database. Note that while the number of partitions per topic is configurable, the replication factor must always be set to 1 because Oracle handles replication at the database level, ensuring the database is the point of failure recovery.\nimport java.util.Collections; import java.util.Properties; import java.util.concurrent.ExecutionException; import org.apache.kafka.clients.admin.Admin; import org.apache.kafka.clients.admin.NewTopic; import org.apache.kafka.common.errors.TopicExistsException; // Implements org.apache.kafka.clients.admin.Admin for Transactional Event Queues import org.oracle.okafka.clients.admin.AdminClient; public class TopicCreator { public static void createTopic(Properties props, String topicName, int partitions) { NewTopic newTopic = new NewTopic(topicName, partitions, (short) 1); try (Admin admin = AdminClient.create(props)) { admin.createTopics(Collections.singletonList(newTopic)) .all() .get(); } catch (ExecutionException | InterruptedException e) { // Handle case where topic already exists or handle other exceptions as appropriate. if (e.getCause() instanceof TopicExistsException) { System.out.println(\"Topic already exists, skipping creation\"); } else { throw new RuntimeException(e); } } } } Producing Messages Similar to standard Kafka producers, producers using the Kafka Java Client for Oracle Database Transactional Event Queues must configure key and value serializers. The following snippet adds a standard StringSerializer for both, though you can provide custom implementations as needed. You may also use standard Kafka properties like enable.idempotence when configuring producers for Oracle Database Transactional Event Queues to ensure messages are not duplicated in the event of retries.\n// Assume props is a configured Properties object for Oracle Database props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // Configure additional properties as needed. props.put(\"enable.idempotence\", \"true\"); The following Java snippet creates a producer using the Kafka Java Client for Oracle Database Transactional Event Queues, and sends a message. The org.oracle.okafka.clients.producer.KafkaProducer class works like a standard Kafka producer but integrates with TxEventQ and supports transactional messaging.\n// Create the producer Producer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); // Send a message producer.send(new ProducerRecord\u003c\u003e(\"my_topic\", \"my first message!\")); The Kafka Java Client for Oracle Database Transactional Event Queues supports all variations of producer.send() including partitioning logic, message keys, and headers.\nConsuming Messages Consumers created using the Kafka Java Client for Oracle Database Transactional Event Queues use standard Kafka properties, and must specify key and value serializers. The following Java snippet configures a Properties object for a consumer:\n// Assume props is already configured with Oracle Database authentication properties as detailed earlier. props.put(\"group.id\", \"MY_CONSUMER_GROUP\"); props.put(\"enable.auto.commit\", \"false\"); props.put(\"max.poll.records\", 2000); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"earliest\"); The org.oracle.okafka.clients.consumer.KafkaConsumer class implements the Kafka Java Client Consumer interface, allowing you to consume and process messages with a familiar API:\n// Create the consumer Consumer\u003cString, String\u003e consumer = new KafkaConsumer\u003c\u003e(props); // Subscribe to topics consumer.subscribe(List.of(\"my_topic\")); // Poll for records. Typically done in a loop ConsumerRecords\u003cString, String\u003e records = consumer.poll(Duration.ofMillis(500)); Transactional Messaging The Kafka Java Client for Oracle Database Transactional Event Queues provides full support for transactional messaging capabilities, supporting database commit, rollback, and query capabilities. See the Transactional Messaging section for comprehensive code examples.\nCustom Serializers Kafka clients handle complex message payloads using custom serializers to convert objects to and from binary message data, allowing automatic binary-object conversions. We’ll implement a serializer and deserializer for Oracle Database’s native binary JSON format, OSON, as an example. Producers and consumers configured with the OSON serializer/deserializer will be able to read and write OSON data using Java objects.\nTo add necessary Oracle JSON dependencies to your project, use the Oracle JSON Collections Starter Maven package:\n\u003cdependency\u003e \u003cgroupId\u003ecom.oracle.database.spring\u003c/groupId\u003e \u003cartifactId\u003eoracle-spring-boot-starter-json-collections\u003c/artifactId\u003e \u003cversion\u003e${oracle-starters.version}\u003c/version\u003e \u003c/dependency\u003e Or, if you’re using Gradle:\nimplementation \"com.oracle.database.spring:oracle-spring-boot-starter-json-collections:${oracleStartersVersion}\" Serializer Implementation The OSON serializer implementation uses the JSONB class from the Oracle JSON starter to convert an arbitrary Java object into OSON:\npackage com.example; import com.oracle.spring.json.jsonb.JSONB; import org.apache.kafka.common.serialization.Serializer; /** * The JSONBSerializer converts java objects to a JSONB byte array. * @param \u003cT\u003e serialization type. */ public class JSONBSerializer\u003cT\u003e implements Serializer\u003cT\u003e { private final JSONB jsonb; public JSONBSerializer(JSONB jsonb) { this.jsonb = jsonb; } @Override public byte[] serialize(String s, T obj) { return jsonb.toOSON(obj); } } The serializer can be added to a producer as a value serializer for a Java object like so:\nJSONB json = new JSONB(new OracleJsonFactory(), (YassonJsonb) JsonbBuilder.create()); Serializer\u003cMyPOJO\u003e keySerializer = new StringSerializer(); Serializer\u003cMyPOJO\u003e valueSerializer = new JSONBSerializer\u003c\u003e(jsonb); Producer\u003cString, MyPOJO\u003e producer = KafkaProducer\u003c\u003e(props, keySerializer, valueSerializer); Deserializer Implementation The OSON deserializer implementation uses the JSONB class from the Oracle JSON starter to convert binary OSON data into a Java object:\npackage com.example; import java.nio.ByteBuffer; import com.oracle.spring.json.jsonb.JSONB; import org.apache.kafka.common.serialization.Deserializer; /** * The JSONBDeserializer converts JSONB byte arrays to java objects. * @param \u003cT\u003e deserialization type */ public class JSONBDeserializer\u003cT\u003e implements Deserializer\u003cT\u003e { private final JSONB jsonb; private final Class\u003cT\u003e clazz; public JSONBDeserializer(JSONB jsonb, Class\u003cT\u003e clazz) { this.jsonb = jsonb; this.clazz = clazz; } @Override public T deserialize(String s, byte[] bytes) { return jsonb.fromOSON(ByteBuffer.wrap(bytes), clazz); } } The deserializer can be added to a consumer as a value deserializer for a Java object like so:\nJSONB json = new JSONB(new OracleJsonFactory(), (YassonJsonb) JsonbBuilder.create()); Deserializer\u003cMyPOJO\u003e keyDeserializer = new StringDeserializer(); Deserializer\u003cMyPOJO\u003e valueDeserializer = new JSONBDeserializer\u003c\u003e(jsonb, MyPOJO.class); Consumer\u003cString, MyPOJO\u003e consumer = new KafkaConsumer\u003c\u003e(props, keyDeserializer, valueDeserializer);",
    "description": "This section provides developer-friendly examples using the Kafka Java Client for Oracle Database Transactional Event Queues. The Kafka Java client implements Kafka client interfaces, allowing you to use familiar Kafka Java APIs with Oracle Database Transactional Event Queues. You’ll learn how to authenticate to Oracle Database using SSL with Oracle Wallet or PLAINTEXT, create topics, produce messages, and consume messages using Java Kafka clients.\nKafka Java Client for Oracle Database Transactional Event Queues To get started using the client, add the dependency for the Kafka Java Client for Oracle Database Transactional Event Queues to your project. If you’re using Maven:",
    "tags": [],
    "title": "Developing With Kafka APIs",
    "uri": "/microservices-datadriven/transactional-event-queues/kafka/developing-with-kafka/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "Oracle Database Transactional Event Queues (TxEventQ) offers comprehensive integration with Apache Kafka, providing developers with a powerful and flexible messaging platform. This module explores the synergy between TxEventQ and Kafka, covering essential concepts and practical implementations. In this module, we’ll refer to queues as topics when working with TxEventQ and Kafka.\nThroughout this module, we’ll explore practical examples using Java code and SQL commands to demonstrate:\nCreating and managing topics using Kafka APIs with TxEventQ Producing and consuming messages using Kafka client libraries Implementing transactional messaging with database operations Utilizing Kafka REST APIs for TxEventQ message handling Configuring and using Kafka connectors for TxEventQ By the end of this module, you’ll have a comprehensive understanding of how to leverage Oracle TxEventQ’s Kafka compatibility features to build robust, scalable, and event-driven applications.\nKafka and TxEventQ Concepts TxEventQ and Kafka share several common architectural concepts, making it easier for developers familiar with Kafka to work with TxEventQ. Key concepts include:\nTopics: Logical channels for message streams Partitions: Subdivisions of topics for parallel processing Offsets: Unique identifiers for messages within a partition Hashing keys: Deterministic message routing within partitions Brokers: Oracle Database servers hosting topics and partitions Ordering: TxEventQ maintains message ordering within partitions, similar to Kafka. Developers can implement partition-level subscribers to ensure ordered message processing. Developing with Kafka APIs on TxEventQ TxEventQ supports Kafka Java APIs through the Kafka Java Client for Oracle Database Transactional Event Queues, allowing developers to leverage their existing Kafka knowledge when developing with TxEventQ. Key operations include:\nAuthenticating to Oracle Database with Kafka APIs Creating topics and partitions using Kafka Admin Producing messages to topics with Kafka Producers Consuming messages from topics using Kafka Consumers Kafka Connectors Oracle offers a Kafka connector for TxEventQ, enabling seamless integration of messages from both platforms. These connectors allow:\nSyncing messages from Kafka topics to TxEventQ queues Sourcing messages from TxEventQ for consumption by Kafka clients Transactional Messaging One of TxEventQ’s unique features is its ability to combine messaging and database operations within a single transaction. This capability, often referred to as the “transactional outbox” pattern, ensures data consistency across microservices. We’ll explore this pattern through the Kafka Java Client for Oracle Database Transactional Event Queues.\nSpring Boot Integration Spring Boot integration is supported through the Kafka Java Client for Oracle Database Transactional Event Queues’ Spring Boot Starter. You can read more about the starter in the Spring Boot Kafka section.",
    "description": "Oracle Database Transactional Event Queues (TxEventQ) offers comprehensive integration with Apache Kafka, providing developers with a powerful and flexible messaging platform. This module explores the synergy between TxEventQ and Kafka, covering essential concepts and practical implementations. In this module, we’ll refer to queues as topics when working with TxEventQ and Kafka.\nThroughout this module, we’ll explore practical examples using Java code and SQL commands to demonstrate:\nCreating and managing topics using Kafka APIs with TxEventQ Producing and consuming messages using Kafka client libraries Implementing transactional messaging with database operations Utilizing Kafka REST APIs for TxEventQ message handling Configuring and using Kafka connectors for TxEventQ By the end of this module, you’ll have a comprehensive understanding of how to leverage Oracle TxEventQ’s Kafka compatibility features to build robust, scalable, and event-driven applications.",
    "tags": [],
    "title": "Kafka APIs",
    "uri": "/microservices-datadriven/transactional-event-queues/kafka/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Spring Boot Integration",
    "content": "This section provides information on getting started with the Kafka Java Client for Oracle Database Transactional Event Queues in a Spring Boot application.\nYou can learn more about the Kafka APIs of Oracle Database Transactional Event Queues in the Kafka chapter.\nProject Dependencies The Kafka Java Client for Oracle Database Transactional Event Queues Spring Boot Starter pulls in all necessary dependencies for developers to work with Transactional Event Queues’ Kafka Java API using Spring Boot.\nIf you’re using Maven, add the following dependency to your project:\n\u003cdependency\u003e \u003cgroupId\u003ecom.oracle.database.spring\u003c/groupId\u003e \u003cartifactId\u003eoracle-spring-boot-starter-okafka\u003c/artifactId\u003e \u003cversion\u003e${oracle-database-starters.version}\u003c/version\u003e \u003c/dependency\u003e Or, if you’re using Gradle:\nimplementation 'com.oracle.database.spring:oracle-spring-boot-starter-okafka:${oracleDatabaseStartersVersion}' Configuring the Starter We’ll create a simple Spring Configuration class that pulls in properties for configuring the Kafka Java Client for Oracle Database Transactional Event Queues. You can read more about these configuration properties in the Developing With Kafka APIs section.\n@Configuration public class OKafkaConfiguration { public static final String TOPIC_NAME = \"OKAFKA_SAMPLE\"; @Value(\"${ojdbc.path}\") private String ojdbcPath; @Value(\"${bootstrap.servers}\") private String bootstrapServers; // We use the default 23ai Free service name @Value(\"${service.name:freepdb1}\") private String serviceName; // Use plaintext for containerized, local, or insecure databases. // Use of SSL with Oracle Wallet is otherwise recommend, such as for Autonomous Database. @Value(\"${security.protocol:PLAINTEXT}\") private String securityProtocol; @Bean @Qualifier(\"okafkaProperties\") public Properties kafkaProperties() { return OKafkaUtil.getConnectionProperties(ojdbcPath, bootstrapServers, securityProtocol, serviceName); } } Configuring a Producer Bean We can now configure a sample producer bean using the org.oracle.okafka.clients.producer.KafkaProducer class:\n@Bean @Qualifier(\"sampleProducer\") public Producer\u003cString, String\u003e sampleProducer() throws IOException { // Create the OKafka Producer. Properties props = kafkaProperties(); props.put(\"enable.idempotence\", \"true\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // Note the use of the org.oracle.okafka.clients.producer.KafkaProducer class, for Oracle TxEventQ. return new KafkaProducer\u003cString, String\u003e(props); } The Producer bean can be autowired into Spring components to write messages to Oracle Database Transactional Event Queue topics. For a complete example of writing data to topics, see Producing messages to Kafka Topics.\nConfiguring a Consumer Bean Next, we’ll configure a sample consumer bean using the org.oracle.okafka.clients.consumer.KafkaConsumer class:\n@Bean @Qualifier(\"sampleConsumer\") public Consumer\u003cString, String\u003e sampleConsumer() { // Create the OKafka Consumer. Properties props = kafkaProperties(); props.put(\"group.id\" , \"MY_CONSUMER_GROUP\"); props.put(\"enable.auto.commit\",\"false\"); props.put(\"max.poll.records\", 2000); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"earliest\"); // Note the use of the org.oracle.okafka.clients.consumer.KafkaConsumer class, for Oracle TxEventQ. return new KafkaConsumer\u003c\u003e(props); } The Consumer bean can be autowired into Spring components to poll messages from Oracle Database Transactional Event Queue topics. For a complete consumer example, see Consuming messages from Kafka topics.\nSample Application Code The following samples provide application code using the Spring starter.\nOracle Spring Boot Sample for JSON Events and the Kafka Java Client for Oracle Database Transactional Event Queues Oracle Spring Boot Sample for the Kafka Java Client for Oracle Database Transactional Event Queues",
    "description": "This section provides information on getting started with the Kafka Java Client for Oracle Database Transactional Event Queues in a Spring Boot application.\nYou can learn more about the Kafka APIs of Oracle Database Transactional Event Queues in the Kafka chapter.\nProject Dependencies The Kafka Java Client for Oracle Database Transactional Event Queues Spring Boot Starter pulls in all necessary dependencies for developers to work with Transactional Event Queues’ Kafka Java API using Spring Boot.",
    "tags": [],
    "title": "Kafka Starter",
    "uri": "/microservices-datadriven/transactional-event-queues/spring-boot/kafka/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Getting Started",
    "content": "This section covers the management of Transactional Event Queues, including the grants and roles required to use queues, steps to create, start, and stop queues across different programming languages and APIs.\nDatabase Permissions for Transactional Event Queues Permissions for SQL Packages Permissions for Users of Kafka APIs Creating, Starting, and Stopping Queues DBMS_AQADM SQL Package Kafka APIs Java with JMS Python .NET JavaScript Oracle REST Data Services Database Permissions for Transactional Event Queues Permissions for SQL Packages For management of queues using Transactional Event Queue APIs in SQL or other languages, the following permissions are recommended for users managing queues:\n-- Grant tablespace as appropriate to your TxEventQ user grant resource, connect to testuser; grant aq_user_role to testuser; grant execute on dbms_aq to testuser; grant execute on dbms_aqadm to testuser; grant execute on dbms_aqin to testuser; grant execute on dbms_aqjms to testuser; grant execute on dbms_teqk to testuser; Permissions for Users of Kafka APIs If your database user is interacting with Transactional Event Queues via Kafka APIs and the Kafka Java Client for Oracle Database Transactional Event Queues, the following permissions are recommended for users managing topics and messages:\n-- Grant tablespace as appropriate to your TxEventQ user grant resource, connect to testuser; grant aq_user_role to testuser; grant execute on dbms_aq to testuser; grant execute on dbms_aqadm to testuser; grant select on gv_$session to testuser; grant select on v_$session to testuser; grant select on gv_$instance to testuser; grant select on gv_$listener_network to testuser; grant select on sys.dba_rsrc_plan_directives to testuser; grant select on gv_$pdbs to testuser; grant select on user_queue_partition_assignment_table to testuser; exec dbms_aqadm.grant_priv_for_rm_plan('testuser'); Creating, Starting, and Stopping Queues DBMS_AQADM SQL Package The DBMS_AQADM SQL package provides procedures for the management of Transactional Event Queues.\nA queue can be created using the DBMS_AQADM.CREATE_TRANSACTIONAL_EVENT_QUEUE procedure. Queues must be started with the DBMS_AQADM.START_QUEUE procedure before they can be used for enqueue and dequeue.\nBelow is an example of creating and starting a queue using DBMS_AQADM procedures.\nbegin -- create the Transactional Event Queue dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'my_queue', -- when multiple_consumers is true, this will create a pub/sub \"topic\" - the default is false. multiple_consumers =\u003e false ); -- start the Transactional Event Queue dbms_aqadm.start_queue( queue_name =\u003e 'my_queue' ); end; / Use the DBMS_AQADM.ALTER_TRANSACTIONAL_EVENT_QUEUE procedure to modify an existing queue. This procedure can be used to change queue retries, comment the queue, modify queue properties, and change the queue’s replication mode.\nThe following SQL script adds a comment to an existing queue.\nbegin dbms_aqadm.alter_transactional_event_queue( queue_name =\u003e 'my_queue', comment =\u003e 'for testing purposes' ); end; / The DBMS_AQADM.PURGE_QUEUE procedure is used to clear messages from a queue.\nbegin dbms_aqadm.purge_queue( queue_name =\u003e 'my_queue' ); end; / Use the DBMS_AQADM.STOP_QUEUE procedure to stop a queue. A queue must be stopped before it can be dropped using the DBMS_AQADM.DROP_TRANSACTIONAL_EVENT_QUEUE procedure.\nbegin dbms_aqadm.stop_queue( queue_name =\u003e 'my_queue' ); dbms_aqadm.drop_transactional_event_queue( queue_name =\u003e 'my_queue' ); end; / To view the current queues in the user schema, query the user_queues table.\nselect * from user_queues; You should see queue data similar to the following, for the queues available on your specific database schema.\nNAME QUEUE_TABLE QID QUEUE_TYPE MAX_RETRIES RETRY_DELAY ENQUEUE_ENABLED DEQUEUE_ENABLED RETENTION USER_COMMENT NETWORK_NAME SHARDED QUEUE_CATEGORY RECIPIENTS JSON_QUEUE JSON_QUEUE 72604 NORMAL_QUEUE 5 0 YES YES 0 null null TRUE Transactional Event Queue SINGLE CUSTOM_TYPE_QUEUE CUSTOM_TYPE_QUEUE 72535 NORMAL_QUEUE 5 0 YES YES 0 null null TRUE Transactional Event Queue SINGLE MY_QUEUE MY_QUEUE 73283 NORMAL_QUEUE 5 0 YES YES 0 null null TRUE Transactional Event Queue SINGLE Kafka APIs You can use standard Kafka APIs to create a topic with the Kafka Java Client for Oracle Database Transactional Event Queues. The following code configures connection properties for Oracle Database and creates a topic using the org.oracle.okafka.clients.admin.AdminClient class, which implements the org.apache.kafka.clients.admin.Admin interface.\n// Oracle Database Connection properties Properties props = new Properties(); // Use your database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Choose PLAINTEXT or SSL as appropriate for your database connection props.put(\"security.protocol\", \"SSL\"); // Your database server props.put(\"bootstrap.servers\", \"my-db-server\"); // Path to directory containing ojdbc.properties // If using Oracle Wallet, this directory must contain the unzipped wallet props.put(\"oracle.net.tns_admin\", \"/my/path/\"); NewTopic topic = new NewTopic(\"my_topic\", 1, (short) 1); try (Admin admin = AdminClient.create(props)) { admin.createTopics(Collections.singletonList(topic)) .all() .get(); } catch (ExecutionException | InterruptedException e) { // Handle topic creation exception } Java with JMS The oracle.jms Java package includes several APIs for managing queues, enqueuing, and dequeuing messages using JMS. The Oracle Spring Boot Starter for AqJms provides a comprehensive set of dependencies to get started using the Java JMS API with Transactional Event Queues. The Spring Boot section includes a detailed producer consumer example using Spring JMS for Transactional Event Queues.\nPython When using Python, the python-oracledb package is helpful for enqueuing and dequeuing messages. Queue creation and management should be handled by the Python Database Driver or a SQL script run by a database administrator.\n.NET Before you can use Transactional Event queues from .NET, you need to create and start queues using the appropriate PL/SQL procedures. The OracleAQMessage class can be used to enqueue and dequeue messages from queues.\nJavaScript Using the node-oracledb package, an AqQueue class can be created from a connection for enqueuing and dequeuing messages. Queue creation and management should be handled by a database administrator.\nOracle REST Data Services Oracle REST Data Services (ORDS) is a Java Enterprise Edition (Java EE) based data service that provides enhanced security, file caching features, and RESTful Web Services. Oracle REST Data Services also increases flexibility through support for deployment in standalone mode, as well as using servers like Oracle WebLogic Server and Apache Tomcat.\nWith ORDS, REST APIs can be used to manage Transactional Event Queues, including creating queues, producing and consuming messages.",
    "description": "This section covers the management of Transactional Event Queues, including the grants and roles required to use queues, steps to create, start, and stop queues across different programming languages and APIs.\nDatabase Permissions for Transactional Event Queues Permissions for SQL Packages Permissions for Users of Kafka APIs Creating, Starting, and Stopping Queues DBMS_AQADM SQL Package Kafka APIs Java with JMS Python .NET JavaScript Oracle REST Data Services Database Permissions for Transactional Event Queues Permissions for SQL Packages For management of queues using Transactional Event Queue APIs in SQL or other languages, the following permissions are recommended for users managing queues:",
    "tags": [],
    "title": "Queue Management",
    "uri": "/microservices-datadriven/transactional-event-queues/getting-started/queue-management/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Kafka APIs",
    "content": "This section introduces Kafka connectors to connect Oracle Database Transactional Event Queues with other data and messaging systems, like Apache Kafka topics.\nKafka Connectors The Kafka connectors for Oracle Database Transactional Event Queues provide the capability to sync message data to/from Kafka topics.\nThe Sink Connector reads from Kafka and publishes messages to Oracle Database Transactional Event Queues. The Source Connector reads from an Oracle Database Transactional Event Queues topic and publishes messages to a Kafka topic.",
    "description": "This section introduces Kafka connectors to connect Oracle Database Transactional Event Queues with other data and messaging systems, like Apache Kafka topics.\nKafka Connectors The Kafka connectors for Oracle Database Transactional Event Queues provide the capability to sync message data to/from Kafka topics.\nThe Sink Connector reads from Kafka and publishes messages to Oracle Database Transactional Event Queues. The Source Connector reads from an Oracle Database Transactional Event Queues topic and publishes messages to a Kafka topic.",
    "tags": [],
    "title": "Kafka Connectors",
    "uri": "/microservices-datadriven/transactional-event-queues/kafka/kafka-connectors/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Getting Started",
    "content": "This section explains message operations using queues, topics, and different programming interfaces (SQL, Java, Spring JMS, and more). You’ll learn how to enqueue, dequeue, and manage messages effectively.\nEnqueue and Dequeue, or Produce and Consume Queues Topics Enqueuing and Dequeuing with SQL Kafka Producers and Consumers Kafka Producer Kafka Consumer Enqueuing and Dequeuing with JMS JMS APIs Message Operations in Other Languages and APIs Message Expiry and Exception Queues Message Delay Message Priority Transactional Messaging: Combine Messaging with Database Queries Enqueue and Dequeue, or Produce and Consume Queues When working with queues, the preferred terms for adding and retrieving messages from Transactional Event Queues are enqueue and dequeue.\nTopics When using topics, the preferred terms are produce and consume. A service that writes data to a topic is called a producer, and a service that reads data from a topic is called a consumer.\nEnqueuing and Dequeuing with SQL To write data to a queue, the queue must be both created and started. See Queue Management for creating and starting queues.\nThe following SQL script enqueues a message for a queue with the JSON payload type:\ndeclare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; msg_id raw(16); message json; body varchar2(200) := '{\"content\": \"my first message\"}'; begin select json(body) into message; dbms_aq.enqueue( queue_name =\u003e 'json_queue', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e msg_id ); end; / Next, we’ll dequeue the message and print it to the console:\ndeclare dequeue_options dbms_aq.dequeue_options_t; message_properties dbms_aq.message_properties_t; msg_id raw(16); message json; message_buffer varchar2(500); begin dequeue_options.navigation := dbms_aq.first_message; dequeue_options.wait := dbms_aq.no_wait; dbms_aq.dequeue( queue_name =\u003e 'json_queue', dequeue_options =\u003e dequeue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e msg_id ); select json_value(message, '$.content') into message_buffer; dbms_output.put_line('message: ' || message_buffer); end; / You may also query a message’s content by ID from the underlying queue table:\nselect q.user_data from json_queue q where msgid = '\u003cmsg id\u003e'; -- Query using the message ID -- {\"content\":\"my first message\"} Kafka Producers and Consumers To produce or consume topic data, the topic must be created. See Queue Management for a topic creation example.\nKafka Producer The following Java snippet creates an org.oracle.okafka.clients.producer.KafkaProducer instance capable of producing data to Transactional Event Queue topics. Note the use of Oracle Database connection properties, and Kafka producer-specific properties like enable.idempotence and key.serializer.\nThe org.oracle.okafka.clients.producer.KafkaProducer class implements the org.apache.kafka.clients.producer.Producer interface, allowing it to be used in place of a Kafka Java client producer.\nProperties props = new Properties(); // Use your database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Choose PLAINTEXT or SSL as appropriate for your database connection props.put(\"security.protocol\", \"SSL\"); // Your database server props.put(\"bootstrap.servers\", \"my-db-server\"); // Path to directory containing ojdbc.properties // If using Oracle Wallet, this directory must contain the unzipped wallet (such as in sqlnet.ora) props.put(\"oracle.net.tns_admin\", \"/my/path/\"); props.put(\"enable.idempotence\", \"true\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); Producer\u003cString, String\u003e okafkaProducer = new KafkaProducer\u003c\u003e(props); The following Java class produces a stream of messages to a topic, using the Kafka Java Client for Oracle Database Transactional Event Queues. Note that the implementation does not use any Oracle-specific classes, only Kafka interfaces. This allows developers to drop in an org.oracle.okafka.clients.producer.KafkaProducer instance without code changes.\nimport java.util.stream.Stream; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; public class SampleProducer\u003cT\u003e implements Runnable, AutoCloseable { private final Producer\u003cString, T\u003e producer; private final String topic; private final Stream\u003cT\u003e inputs; public SampleProducer(Producer\u003cString, T\u003e producer, String topic, Stream\u003cT\u003e inputs) { this.producer = producer; this.topic = topic; this.inputs = inputs; } @Override public void run() { inputs.forEach(t -\u003e { System.out.println(\"Produced record: \" + t); producer.send(new ProducerRecord\u003c\u003e(topic, t)); }); } @Override public void close() throws Exception { if (this.producer != null) { producer.close(); } } } Kafka Consumer The following Java snippet creates an org.oracle.okafka.clients.consumer.KafkaConsumer instance capable of records from Transactional Event Queue topics. Note the use of Oracle Database connection properties, and Kafka consumer-specific properties like group.id and max.poll.records.\nThe org.oracle.okafka.clients.consumer.KafkaConsumer class implements the org.apache.kafka.clients.consumer.Consumer interface, allowing it to be used in place of a Kafka Java client consumer.\nProperties props = new Properties(); // Use your database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Choose PLAINTEXT or SSL as appropriate for your database connection props.put(\"security.protocol\", \"SSL\"); // Your database server props.put(\"bootstrap.servers\", \"my-db-server\"); // Path to directory containing ojdbc.properties // If using Oracle Wallet, this directory must contain the unzipped wallet (such as in sqlnet.ora) props.put(\"oracle.net.tns_admin\", \"/my/path/\"); props.put(\"group.id\" , \"MY_CONSUMER_GROUP\"); props.put(\"enable.auto.commit\",\"false\"); props.put(\"max.poll.records\", 2000); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); Consumer\u003cString, String\u003e okafkaConsumer = new KafkaConsumer\u003c\u003e(props); The following Java class consumes messages from a topic, using the Kafka Java Client for Oracle Database Transactional Event Queues. Like the producer example, the consumer only does not use any Oracle classes, only Kafka interfaces.\nimport java.time.Duration; import java.util.List; import org.apache.kafka.clients.consumer.Consumer; import org.apache.kafka.clients.consumer.ConsumerRecords; public class SampleConsumer\u003cT\u003e implements Runnable, AutoCloseable { private final Consumer\u003cString, T\u003e consumer; private final String topic; public SampleConsumer(Consumer\u003cString, T\u003e consumer, String topic) { this.consumer = consumer; this.topic = topic; } @Override public void run() { consumer.subscribe(List.of(topic)); while (true) { ConsumerRecords\u003cString, T\u003e records = consumer.poll(Duration.ofMillis(100)); System.out.println(\"Consumed records: \" + records.count()); processRecords(records); // Commit records when done processing. consumer.commitAsync(); } } private void processRecords(ConsumerRecords\u003cString, T\u003e records) { // Application implementation of record processing. } @Override public void close() throws Exception { if (consumer != null) { consumer.close(); } } } Enqueuing and Dequeuing with JMS JMS (Java Message Service) provides a standard way to enqueue and dequeue messages. This section shows how to use plain Java JMS APIs and Spring JMS integration using the oracle.jms Java package and the Oracle Spring Boot Starter for AqJms.\nJMS APIs The AQJmsFactory class is used to create a JMS ConnectionFactory, from a Java DataSource or connection parameters. Once configured, the JMS ConnectionFactory instance can be used with standard JMS APIs to produce and consume messages.\nThe following Java snippet uses a JMS ConnectionFactory to produce a text message.\nDataSource ds = // Configure the Oracle Database DataSource according to your database connection information ConnectionFactory cf = AQjmsFactory.getConnectionFactory(ds); try (Connection conn = cf.createConnection()) { Session session = conn.createSession(); Queue myQueue = session.createQueue(\"my_queue\"); MessageProducer producer = session.createProducer(myQueue); producer.send(session.createTextMessage(\"Hello World\")); } The following Java snippet uses a JMS ConnectionFactory to consume a text message.\nDataSource ds = // Configure the Oracle Database DataSource according to your database connection information ConnectionFactory cf = AQjmsFactory.getConnectionFactory(ds); try (Connection conn = cf.createConnection()) { Session session = conn.createSession(); Queue myQueue = session.createQueue(\"my_queue\"); MessageConsumer consumer = session.createConsumer(myQueue); conn.start(); Message msg = consumer.receive(10000); // Wait for 10 seconds if (msg != null \u0026\u0026 msg instanceof TextMessage) { TextMessage textMsg = (TextMessage) msg; System.out.println(\"Received message: \" + textMsg.getText()); } } Message Operations in Other Languages and APIs For Python, Javascript, .NET, and ORDS, refer to the respective documentation for code samples:\nPython JavaScript .NET ORDS REST APIs Message Expiry and Exception Queues When enqueuing a message, you can specify an expiration time using the expiration attribute of the message_properties object. This sets the number of seconds during which the message is available for dequeuing. Messages that exceed their expiration time are automatically moved to an exception queue for further processing or inspection (including queries or dequeue operations). The exception queue contains any expired or failed messages, and uses the same underlying table as the main queue.\nThe following SQL script creates a queue for JMS payloads, and an associated exception queue for failed or expired messages.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'my_queue', multiple_consumers =\u003e false ); dbms_aqadm.start_queue( queue_name =\u003e 'my_queue' ); dbms_aqadm.create_eq_exception_queue( queue_name =\u003e 'my_queue', exception_queue_name =\u003e 'my_queue_eq' ); end; The following SQL script enqueues a JMS payload, configuring the expiry time so that after 60 seconds, the message is moved to the exception queue.\ndeclare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; message_handle raw(16); message sys.aq$_jms_text_message; begin message := sys.aq$_jms_text_message.construct(); message.set_text('this is my message'); message_properties.expiration := 60; -- message expires in 60 seconds dbms_aq.enqueue( queue_name =\u003e 'my_queue', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e message_handle ); commit; end; / Message Delay When enqueuing a message, you can specify a delay (in seconds) before the message becomes available for dequeuing. Message delay allows you to schedule messages to be available for consumers after a specified time, and is configured using the delay attribute of the message_properties object.\nWhen enqueuing delayed messages, the DELIVERY_TIME column will be configured with the date the message is available for consumers.\ndeclare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; message_handle raw(16); message sys.aq$_jms_text_message; begin message := sys.aq$_jms_text_message.construct(); message.set_text('this is my message'); message_properties.delay := 7*24*60*60; -- Delay for 7 days dbms_aq.enqueue( queue_name =\u003e 'my_queue', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e message_handle ); commit; end; / Message Priority When enqueuing a message, you can specify its priority using the priority attribute of the message_properties object. This attribute allows you to control the order in which messages are dequeued. The lower a message’s priority number, the higher the message’s precedence for consumers.\nWhen enqueuing prioritized messages, the PRIORITY column in the queue table will be populated with the priority number.\ndeclare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; message_handle raw(16); message sys.aq$_jms_text_message; begin message := sys.aq$_jms_text_message.construct(); message.set_text('this is my message'); message_properties.priority := 1; -- A lower number indicates higher priority dbms_aq.enqueue( queue_name =\u003e 'my_queue', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e message_handle ); commit; end; / Transactional Messaging: Combine Messaging with Database Queries Enqueue and dequeue operations occur within database transactions, allowing developers to combine database DML with messaging operations. This is particularly useful when the message contains data relevant to other tables or services within your schema.\nIn the following example, a DML operation (an INSERT query) is combined with an enqueue operation in the same transaction. If the enqueue operation fails, the INSERT is rolled back. The orders table serves as the example.\ncreate table orders ( id number generated always as identity primary key, product_id number not null, quantity number not null, order_date date default sysdate ); declare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; msg_id raw(16); message json; body varchar2(200) := '{\"product_id\": 1, \"quantity\": 5}'; product_id number; quantity number; begin -- Convert the JSON string to a JSON object message := json(body); -- Extract product_id and quantity from the JSON object product_id := json_value(message, '$.product_id' returning number); quantity := json_value(message, '$.quantity' returning number); -- Insert data into the orders table insert into orders (product_id, quantity) values (product_id, quantity); -- Enqueue the message dbms_aq.enqueue( queue_name =\u003e 'json_queue', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e msg_id ); commit; exception when others then -- Rollback the transaction on error rollback; dbms_output.put_line('error dequeuing message: ' || sqlerrm); end; / Note: The same pattern applies to the dbms_aq.dequeue procedure, allowing developers to perform DML operations within dequeue transactions.",
    "description": "This section explains message operations using queues, topics, and different programming interfaces (SQL, Java, Spring JMS, and more). You’ll learn how to enqueue, dequeue, and manage messages effectively.\nEnqueue and Dequeue, or Produce and Consume Queues Topics Enqueuing and Dequeuing with SQL Kafka Producers and Consumers Kafka Producer Kafka Consumer Enqueuing and Dequeuing with JMS JMS APIs Message Operations in Other Languages and APIs Message Expiry and Exception Queues Message Delay Message Priority Transactional Messaging: Combine Messaging with Database Queries Enqueue and Dequeue, or Produce and Consume Queues When working with queues, the preferred terms for adding and retrieving messages from Transactional Event Queues are enqueue and dequeue.",
    "tags": [],
    "title": "Message Operations",
    "uri": "/microservices-datadriven/transactional-event-queues/getting-started/message-operations/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Performance and Observability",
    "content": "Performance tuning is critical for ensuring the efficiency, reliability, and responsiveness of TxEventQ event processing. As systems scale and the volume of events increases, poor optimizations can lead to latency, bottlenecks, or even system failures. By applying the techniques described in this section, TxEventQ administrators can significantly enhance throughput, reduce latency, and improve the stability of event driven applications.\nEvent Streams and Multiple Consumers Creating a Partitioned Kafka Topic Message Cache Optimization Tuning System Parameters DB_BLOCK_SIZE JAVA_POOL_SIZE OPEN_CURSORS PGA_AGGREGATE_TARGET PROCESSES SESSIONS SGA_TARGET STREAMS_POOL_SIZE Event Streams and Multiple Consumers Oracle Database automatically manages the database table partitions of a queue. As queue volume fluctuates, table partitions may be dynamically created as necessary, for example, when the queue table expands due to a message backlog. Once all messages are dequeued and no longer needed, the database table partition is truncated and made available for reuse.\nQueue event streams can be configured on a queue during creation. A higher number of event streams per queue improves dequeue performance through consumer parallelization, but requires additional memory and database resources. When messages are enqueued, each message is routed to a specific event stream of the queue. Messages within an event stream maintain a strict, session-level ordering.\nIf you wish to manually set the number of event streams for a queue, you may do so using the SET_QUEUE_PARAMETER procedure. The following SQL script creates a queue, and configures it to have 5 event streams and key based enqueue. Key based enqueue allows message routing to a specific event stream within a queue by providing a message key at enqueue time.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'my_queue', queue_payload_type =\u003e DBMS_AQADM.JMS_TYPE, multiple_consumers =\u003e true ); -- Set the queue to have 5 event streams dbms_aqadm.set_queue_parameter('my_queue', 'SHARD_NUM', 5); -- The event stream for enqueued messages is determined by the message key dbms_aqadm.set_queue_parameter('my_queue', 'KEY_BASED_ENQUEUE', 1); -- Start the queue dbms_aqadm.start_queue( queue_name =\u003e 'my_queue' ); end; / It is recommended to avoid creating global indexes on the partitioned table that backs a queue. Local indexes are generally not recommended either and may result in performance degradation.\nCreating a Partitioned Kafka Topic The following Java code snippet creates a TxEventQ topic with 5 Kafka partitions (5 TxEventQ event streams) using the Kafka Java Client for Oracle Database Transactional Event Queues. When working with TxEventQ, we’ll refer to Kafka topic partitions as event streams. The my_topic topic may have up to 5 distinct consumer threads per consumer group, increasing parallelization.\nProperties props = // Oracle Database connection properties NewTopic topic = new NewTopic(\"my_topic\", 5, (short) 1); try (Admin admin = AdminClient.create(props)) { admin.createTopics(Collections.singletonList(topic)) .all() .get(); } catch (ExecutionException | InterruptedException e) { // Handle topic creation exception } Note that you can use as many producers per topic as required, though it is recommended to ensure consumers are capable of consuming messages at least as fast as they are produced to avoid creating a message backlog and decreasing message throughput. For additional resources and examples related to developing with Kafka APIs for TxEventQ, see the Kafka chapter.\nMessage Cache Optimization TxEventQ includes a specialized message cache that allows administrators to balance System Global Area (SGA) usage and queue performance. Management of TxEventQ’s SGA usage benefits throughput, reduces latency, and allows greater concurrency. When paired with event streams, message caching reduces the need for certain queries, DML operations, and indexes. The cache is most effective when consumers can keep up with producers, and when it is large enough to store the messages (including payloads) for all consumers and producers for using TxEventQ.\nThe message cache uses the Oracle Database Streams pool. You can fine-tune the memory allocation for the Streams pool using the DBMS_AQADM SET_MIN_STREAMS_POOL and SET_MAX_STREAMS_POOL procedures.\nTuning System Parameters System parameters can be modified as appropriate for your database installation and event volume to improve TxEventQ performance. This section describes key parameters for optimizing TxEventQ and provides a refresher on working with system parameters.\nTo update a system parameter, use the ALTER SYSTEM command. The following statement sets the SGA_TARGET parameter to 2 GB on the running instance and the parameter file.\nalter system set sga_target = 2G scope = both; The scope of a parameter may be MEMORY, SPFILE, or BOTH:\nMEMORY: changes are applied to the running instance without updating the parameter file. SPFILE: changes are applied to the parameter file and are available on the next restart. BOTH: changes are applied dynamically to the running instance, updating both memory and the parameter file. The following query retrieves the current value of the SGA_TARGET parameter:\nselect name, value, isdefault, issys_modifiable, ismodified from v$parameter where name = 'sga_target'; DB_BLOCK_SIZE The default block size of 8k is recommended. The block size of a database cannot be changed after database creation.\nOPEN_CURSORS The OPEN_CURSORS parameter defines the maximum number of open cursors a session can have in Oracle. The default value is 50, and it can be adjusted between 0 and 65535. If you are running a large amount of producers and consumers, it’s essential to set this value high enough to avoid running out of cursors. You can modify OPEN_CURSORS using the ALTER SYSTEM command.\nPGA_AGGREGATE_TARGET The PGA_AGGREGATE_TARGET parameter specifies the target total memory available for the Program Global Area (PGA), which is used by server processes for tasks like sorting and joining. The default is 10 MB or 20% of the SGA size, whichever is greater. It is modifiable and helps optimize SQL operations by adjusting memory for working areas. When set to 0, it changes memory management to manual.\nTo set a hard limit for aggregate PGA memory, use the PGA_AGGREGATE_LIMIT parameter.\nPROCESSES The PROCESSES parameter specifies the maximum number of operating system user processes that can simultaneously connect to the database. It is essential for managing background processes, such as locks, job queues, and parallel execution. Workloads with a large number of producers and consumers may require adjustments to this parameter. Adjusting this value may require reevaluating related parameters like SESSIONS and TRANSACTIONS.\nSESSIONS The SESSIONS parameter defines the maximum number of sessions that can be created in an Oracle database. It is calculated as (1.5 * PROCESSES) + 22 by default and can be modified within a range of 1 to 65,536. It impacts the number of concurrent users and background processes. For Pluggable Databases (PDBs), the SESSIONS parameter is adjustable but cannot exceed the root container’s value. You should adjust the number based on your expected connections and background processes, including queue workloads.\nSGA_TARGET The SGA_TARGET parameter specifies the total size of the System Global Area (SGA) in Oracle, allowing automatic memory management of its components like the buffer cache, shared pool, and others. Its value can range from 64 MB to SGA_MAX_SIZE. It is modifiable via ALTER SYSTEM. If set to zero, SGA autotuning is disabled. Systems that make heavy using of message queuing should configure SGA_TARGET to an appropriately large value to get the most out of queue message caching.\nIf SGA_TARGET is specified, then the following memory pools are automatically sized:\nBuffer cache (DB_CACHE_SIZE) Shared pool (SHARED_POOL_SIZE) Large pool (LARGE_POOL_SIZE) Java pool (JAVA_POOL_SIZE) Streams pool (STREAMS_POOL_SIZE) Data transfer cache (DATA_TRANSFER_CACHE_SIZE) STREAMS_POOL_SIZE The STREAMS_POOL_SIZE parameter defines the size of the Streams pool, a shared memory area used for TxEventQ and other database features. If SGA_TARGET and STREAMS_POOL_SIZE are both nonzero, Oracle Database Automatic Shared Memory Management uses this value as a minimum for the Streams pool.\nIf both the STREAMS_POOL_SIZE and the SGA_TARGET parameters are set to 0, then the first request for Streams pool memory will transfer 10% of the buffer cache shared pool to the Streams pool.",
    "description": "Performance tuning is critical for ensuring the efficiency, reliability, and responsiveness of TxEventQ event processing. As systems scale and the volume of events increases, poor optimizations can lead to latency, bottlenecks, or even system failures. By applying the techniques described in this section, TxEventQ administrators can significantly enhance throughput, reduce latency, and improve the stability of event driven applications.\nEvent Streams and Multiple Consumers Creating a Partitioned Kafka Topic Message Cache Optimization Tuning System Parameters DB_BLOCK_SIZE JAVA_POOL_SIZE OPEN_CURSORS PGA_AGGREGATE_TARGET PROCESSES SESSIONS SGA_TARGET STREAMS_POOL_SIZE Event Streams and Multiple Consumers Oracle Database automatically manages the database table partitions of a queue. As queue volume fluctuates, table partitions may be dynamically created as necessary, for example, when the queue table expands due to a message backlog. Once all messages are dequeued and no longer needed, the database table partition is truncated and made available for reuse.",
    "tags": [],
    "title": "Performance Tuning",
    "uri": "/microservices-datadriven/transactional-event-queues/observability/tuning/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "Oracle Transactional Event Queues (TxEventQ) features several Spring Boot integrations, allowing application developers to work with TxEventQ using common Spring idioms and starters. This module explores the various integration points and best practices for leveraging TxEventQ within the Spring ecosystem.\nSpring Boot Starter for AQ/JMS The Oracle Spring Boot Starter for AQ/JMS simplifies the integration of TxEventQ with Spring and JMS. Key features include:\nAutomatic configuration of JMS ConnectionFactory Support for transactional message processing Easy setup with Maven or Gradle dependencies Spring Boot Starter for Kafka Java Client for Oracle Database Transactional Event Queues The Spring Boot Starter for the Kafka Java Client for Oracle Database Transactional Event Queues integrates all necessary dependencies to use TxEventQ with the Kafka Java API within a Spring Boot application.\nTxEventQ Spring Cloud Stream Binder Spring Cloud Stream is a Java framework designed for building event-driven microservices backed by scalable, fault-tolerant messaging systems. The Oracle Database Transactional Event Queues (TxEventQ) stream binder implementation enables developers to integrate Oracle’s database messaging platform with Spring Cloud Stream. This integration allows you to keep your data within the converged database while benefiting from a functional message interface.",
    "description": "Oracle Transactional Event Queues (TxEventQ) features several Spring Boot integrations, allowing application developers to work with TxEventQ using common Spring idioms and starters. This module explores the various integration points and best practices for leveraging TxEventQ within the Spring ecosystem.\nSpring Boot Starter for AQ/JMS The Oracle Spring Boot Starter for AQ/JMS simplifies the integration of TxEventQ with Spring and JMS. Key features include:\nAutomatic configuration of JMS ConnectionFactory Support for transactional message processing Easy setup with Maven or Gradle dependencies Spring Boot Starter for Kafka Java Client for Oracle Database Transactional Event Queues The Spring Boot Starter for the Kafka Java Client for Oracle Database Transactional Event Queues integrates all necessary dependencies to use TxEventQ with the Kafka Java API within a Spring Boot application.",
    "tags": [],
    "title": "Spring Boot Integration",
    "uri": "/microservices-datadriven/transactional-event-queues/spring-boot/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Spring Boot Integration",
    "content": "Spring Cloud Stream is a Java framework for building event-driven microservices backed by a scalable, fault-tolerant messaging system. The Oracle Database Transactional Event Queues Stream Binder allows developers to leverage Oracle’s database messaging platform within the Spring Cloud Stream ecosystem.\nThis section covers the key features of the Spring Cloud Stream Binder for Oracle Database Transactional Event Queues and getting started examples for developers.\nKey Features of the Transactional Event Queues Stream Binder The Spring Cloud Stream Binder for Oracle Database Transactional Event Queues provides a high-throughput, reliable messaging platform built directly into the database.\nReal-time messaging with multiple publishers, consumers, and topics — all with a simple functional interface. Convergence of data: Your messaging infrastructure integrates directly with the database, eliminating the need for external brokers. Integration with Spring Cloud Stream provides an interface that’s easy-to-use and quick to get started. Configuring the Transactional Event Queues Stream Binder In this section, we’ll cover how to configure the Transactional Event Queues Stream Binder for a Spring Boot project.\nProject Dependencies To start developing with the Stream Binder, add the spring-cloud-stream-binder-oracle-txeventq dependency to your Maven project:\n\u003cdependency\u003e \u003cgroupId\u003ecom.oracle.database.spring.cloud-stream-binder\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-stream-binder-oracle-txeventq\u003c/artifactId\u003e \u003cversion\u003e${txeventq-stream-binder.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle users, add the following dependency to your project:\nimplementation \"com.oracle.database.spring.cloud-stream-binder:spring-cloud-stream-binder-oracle-txeventq:${txeventqStreamBinderVersion}\" Stream Binder Permissions The database user producing/consuming events with the Stream Binder requires the following database permissions. Modify the username and tablespace grant as appropriate for your application:\n-- Grant tablespace as appropriate to your TxEventQ user grant select_catalog_role to testuser; grant execute on dbms_aq to testuser; grant execute on dbms_aqadm to testuser; grant execute on dbms_aqin to testuser; grant execute on dbms_aqjms_internal to testuser; grant execute on dbms_teqk to testuser; grant execute on DBMS_RESOURCE_MANAGER to testuser; grant select on sys.aq$_queue_shards to testuser; grant select on user_queue_partition_assignment_table to testuser; Stream Binder Database Connection The Stream Binder uses a standard JDBC connection to produce and consume messages. With YAML-style Spring application properties, it’ll look something like this:\nspring: datasource: username: ${USERNAME} password: ${PASSWORD} url: ${JDBC_URL} driver-class-name: oracle.jdbc.OracleDriver type: oracle.ucp.jdbc.PoolDataSourceImpl oracleucp: initial-pool-size: 1 min-pool-size: 1 max-pool-size: 30 connection-pool-name: StreamBinderSample connection-factory-class-name: oracle.jdbc.pool.OracleDataSource Suppliers, Functions and Consumers Spring Cloud Stream uses Java suppliers, functions, and consumers to abstract references to the underlying messaging system (in this case, Transactional Event Queues). To illustrate how this works, we’ll create a basic Supplier, Function, and Consumer with Spring Cloud Stream.\nOur example workflow will use three functional interfaces:\nA Supplier that streams a phrase word-by-word A Function that processes each word from supplier and capitalizes it A Consumer that receives each capitalized word from the function and prints it to stdout Once we’ve implemented the Java interfaces, we’ll wire them together with Spring Cloud Stream.\nSupplier Implementation The following Supplier implementation supplies a phrase word-by-word, indicating when it has processed the whole phrase.\nimport java.util.concurrent.atomic.AtomicBoolean; import java.util.concurrent.atomic.AtomicInteger; import java.util.function.Supplier; public class WordSupplier implements Supplier\u003cString\u003e { private final String[] words; private final AtomicInteger idx = new AtomicInteger(0); private final AtomicBoolean done = new AtomicBoolean(false); public WordSupplier(String phrase) { this.words = phrase.split(\" \"); } @Override public String get() { int i = idx.getAndAccumulate(words.length, (x, y) -\u003e { if (x \u003c words.length - 1) { return x + 1; } done.set(true); return 0; }); return words[i]; } public boolean done() { return done.get(); } } Next, let’s add Spring beans for our Supplier, a Function, and a Consumer. The toUpperCase Function takes a string and capitalizes it, and the stdoutConsumer Consumer prints each string it receives to stdout.\nimport java.util.function.Consumer; import java.util.function.Function; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class StreamConfiguration { // Input phrase for the producer @Value(\"${phrase}\") private String phrase; // Function, Capitalizes each input @Bean public Function\u003cString, String\u003e toUpperCase() { return String::toUpperCase; } // Consumer, Prints each input @Bean public Consumer\u003cString\u003e stdoutConsumer() { return s -\u003e System.out.println(\"Consumed: \" + s); } // Supplier, WordSupplier @Bean public WordSupplier wordSupplier() { return new WordSupplier(phrase); } } Configure Beans with Spring Cloud Stream Returning to the application properties, let’s configure the Spring Cloud Stream bindings for each bean defined previously.\nIn our binding configuration, the wordSupplier Supplier has the toUpperCase Function as a destination, and the stdoutConsumer Consumer reads from toUpperCase. The result of this acyclic configuration is that each word from the phrase is converted to uppercase and sent to stdout.\n# Input phrase for the wordSupplier phrase: \"Spring Cloud Stream simplifies event-driven microservices with powerful messaging capabilities.\" spring: cloud: stream: bindings: wordSupplier-out-0: # wordSupplier output destination: toUpperCase-in-0 group: t1 producer: required-groups: - t1 stdoutConsumer-in-0: # stdoutConsumer input destination: toUpperCase-out-0 group: t1 function: # defines the stream flow, toUppercase bridges # wordSupplier and stdoutConsumer definition: wordSupplier;toUpperCase;stdoutConsumer If you’ve added the prior code to a Spring Boot application, you should see the following messages sent to stdout when it is run:\nConsumed: SPRING Consumed: CLOUD Consumed: STREAM Consumed: SIMPLIFIES Consumed: EVENT-DRIVEN Consumed: MICROSERVICES Consumed: WITH Consumed: POWERFUL Consumed: MESSAGING Consumed: CAPABILITIES.",
    "description": "Spring Cloud Stream is a Java framework for building event-driven microservices backed by a scalable, fault-tolerant messaging system. The Oracle Database Transactional Event Queues Stream Binder allows developers to leverage Oracle’s database messaging platform within the Spring Cloud Stream ecosystem.\nThis section covers the key features of the Spring Cloud Stream Binder for Oracle Database Transactional Event Queues and getting started examples for developers.\nKey Features of the Transactional Event Queues Stream Binder The Spring Cloud Stream Binder for Oracle Database Transactional Event Queues provides a high-throughput, reliable messaging platform built directly into the database.",
    "tags": [],
    "title": "Spring Cloud Stream Binder",
    "uri": "/microservices-datadriven/transactional-event-queues/spring-boot/stream-binder/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Getting Started",
    "content": "This section explains advanced features of Transactional Event Queues, including message propagation between queues and the database, and error handling.\nMessage Propagation Queue to Queue Message Propagation Stopping Queue Propagation Using Database Links Error Handling Message Propagation Messages can be propagated within the same database or across a database link to different queues or topics. Message propagation is useful for workflows that require processing by different consumers or for event-driven actions that need to trigger subsequent processes.\nQueue to Queue Message Propagation Create and start two queues. source will be the source queue, and dest will be the propagated destination queue.\nbegin dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'source', queue_payload_type =\u003e 'JSON', multiple_consumers =\u003e true ); dbms_aqadm.create_transactional_event_queue( queue_name =\u003e 'dest', queue_payload_type =\u003e 'JSON', multiple_consumers =\u003e true ); dbms_aqadm.start_queue( queue_name =\u003e 'source' ); dbms_aqadm.start_queue( queue_name =\u003e 'dest' ); end; / Schedule message propagation so messages from source are propagated to dest, using DBMS_AQADM.SCHEDULE_PROPAGATION procedure.\nbegin dbms_aqadm.schedule_propagation( queue_name =\u003e 'source', destination_queue =\u003e 'dest' ); end; / Let’s enqueue a message into source. We expect this message to be propagated to dest:\ndeclare enqueue_options dbms_aq.enqueue_options_t; message_properties dbms_aq.message_properties_t; msg_id raw(16); message json; body varchar2(200) := '{\"content\": \"this message is propagated!\"}'; begin select json(body) into message; dbms_aq.enqueue( queue_name =\u003e 'source', enqueue_options =\u003e enqueue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e msg_id ); commit; end; / If propagation does not occur, check the JOB_QUEUE_PROCESSES parameter and ensure its value is high enough. If the value is very low, you may need to update it with a larger value:\nalter system set job_queue_processes=10; Stopping Queue Propagation You can stop propagation using the DBMS_AQADM.UNSCHEDULE_PROPAGATION procedure:\nbegin dbms_aqadm.unschedule_propagation( queue_name =\u003e 'source', destination_queue =\u003e 'dest' ); end; / Your can view queue subscribers and propagation schedules from the respective DBA_QUEUE_SCHEDULES and DBA_QUEUE_SUBSCRIBERS system views. These views are helpful for debugging propagation issues, including error messages and schedule status.\nUsing Database Links To propagate messages between databases, a database link from the local database to the remote database must be created. The subscribe and propagation commands must be altered to use the database link.\nbegin dbms_aqadm.schedule_propagation( queue_name =\u003e 'source', -- replace with your database link destination =\u003e 'database_link', -- replace with your remote schema and queue name destination_queue =\u003e 'schema.queue_name' ); end; / Error Handling Error handling is a critical component of message processing, ensuring malformed or otherwise unprocessable messages are handled correctly. Depending on the message payload and exception, an appropriate action should be taken to either replay, discard, or otherwise process the failed message. If a message cannot be dequeued due to errors, it may be moved to the exception queue, if one exists.\nFor errors on procedures like enqueue you may also use the standard SQL exception mechanisms:\ndeclare dequeue_options dbms_aq.dequeue_options_t; message_properties dbms_aq.message_properties_t; msg_id raw(16); message json; message_buffer varchar2(500); begin dequeue_options.navigation := dbms_aq.first_message; dequeue_options.wait := dbms_aq.no_wait; dbms_aq.dequeue( queue_name =\u003e 'json_queue', dequeue_options =\u003e dequeue_options, message_properties =\u003e message_properties, payload =\u003e message, msgid =\u003e msg_id ); select json_value(message, '$.content') into message_buffer; dbms_output.put_line('message: ' || message_buffer); exception when others then rollback; dbms_output.put_line('error dequeuing message: ' || sqlerrm); end; /",
    "description": "This section explains advanced features of Transactional Event Queues, including message propagation between queues and the database, and error handling.\nMessage Propagation Queue to Queue Message Propagation Stopping Queue Propagation Using Database Links Error Handling Message Propagation Messages can be propagated within the same database or across a database link to different queues or topics. Message propagation is useful for workflows that require processing by different consumers or for event-driven actions that need to trigger subsequent processes.",
    "tags": [],
    "title": "Advanced Features",
    "uri": "/microservices-datadriven/transactional-event-queues/getting-started/advanced-features/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "Oracle TxEventQ offers powerful performance tuning and monitoring capabilities. This module explores advanced techniques for optimizing queue performance and enhancing observability.\nTxEventQ Administrative Views TxEventQ provides administrative views for monitoring performance, including insights into message cache statistics, partition level metrics, and subscriber load. This module will dive into accessing and understanding these database views and their content.\nOracle Database Metrics Exporter The Oracle Database Metrics Exporter can be configured to export metrics about TxEventQ, providing access to the real-time broker, producer, and consumer metrics in a Grafana dashboard that allows teams to receive alerts for issues and understand the state of their system.\nPerformance Tuning This module will cover several methods of optimizing TxEventQ performance, including the message cache, Streams pool size, and more.\nBy leveraging these techniques, you can ensure optimal performance and visibility for your TxEventQ implementations.",
    "description": "Oracle TxEventQ offers powerful performance tuning and monitoring capabilities. This module explores advanced techniques for optimizing queue performance and enhancing observability.\nTxEventQ Administrative Views TxEventQ provides administrative views for monitoring performance, including insights into message cache statistics, partition level metrics, and subscriber load. This module will dive into accessing and understanding these database views and their content.\nOracle Database Metrics Exporter The Oracle Database Metrics Exporter can be configured to export metrics about TxEventQ, providing access to the real-time broker, producer, and consumer metrics in a Grafana dashboard that allows teams to receive alerts for issues and understand the state of their system.",
    "tags": [],
    "title": "Performance and Observability",
    "uri": "/microservices-datadriven/transactional-event-queues/observability/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e Kafka APIs",
    "content": "This section provides a detailed walkthrough of transactional messaging using the Kafka Java Client for Oracle Database Transactional Event Queues. You’ll learn how to run, commit, and abort database transactions while using Kafka producers and consumers for transactional messaging.\nKafka Example The KafkaProducer and KafkaConsumer classes implemented by the Kafka Java Client for Oracle Transactional Event Queues provide functionality for transactional messaging, allowing developers to run database queries within a produce or consume transaction.\nTransactional Messaging ensures atomicity between messaging processing and the database, ensuring that if a message is produced the corresponding database operation also commits or is rolled back in case of failure.\nTransactional Produce To configure a transactional producer, configure the org.oracle.okafka.clients.producer.KafkaProducer class with the oracle.transactional.producer=true property.\nOnce the producer instance is created, initialize the producer for transactional management using the producer.initTransactions() method.\nProperties props = new Properties(); // Use your database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Choose PLAINTEXT or SSL as appropriate for your database connection props.put(\"security.protocol\", \"SSL\"); // Your database server props.put(\"bootstrap.servers\", \"my-db-server\"); // Path to directory containing ojdbc.properties // If using Oracle Wallet, this directory must contain the unzipped wallet (such as in sqlnet.ora) props.put(\"oracle.net.tns_admin\", \"/my/path/\"); props.put(\"enable.idempotence\", \"true\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // Enable Transactional messaging with the producer props.put(\"oracle.transactional.producer\", \"true\"); KafkaProducer\u003cString, String\u003e producer = new KafkaProducer\u003c\u003e(props); // Initialize the producer for database transactions producer.initTransactions(); Producer Methods To start a database transaction, use the producer.beginTransaction() method. To commit the transaction, use the producer.commitTransaction() method. To retrieve the current database connection within the transaction, use the producer.getDBConnection() method. To abort the transaction, use the producer.abortTransaction() method. Transactional Produce Example The following Java method takes in an input record and processes it using a transactional producer. On error, the transaction is aborted and neither the DML nor topic produce are committed to the database. Assume the processRecord method does some DML operation with the record, like inserting or updating a table.\npublic void produce(String record) { // 1. Begin the current transaction producer.beginTransaction(); try { // 2. Create the producer record and prepare to send it to a topic ProducerRecord\u003cString, String\u003e pr = new ProducerRecord\u003c\u003e( topic, record ); producer.send(pr); // 3. Use the record in database DML processRecord(record, conn); } catch (Exception e) { // 4. On error, abort the transaction System.out.println(\"Error processing record\", e); producer.abortTransaction(); } // 5. Once complete, commit the transaction. producer.commitTransaction(); System.out.println(\"Processed record\"); } Transactional Consume To configure a transactional consumer, configure the org.oracle.okafka.clients.consumer.KafkaConsumer class with auto.commit=false. Disabling auto-commit allows control of database transactions through the commitSync() and commitAsync() methods.\nProperties props = new Properties(); // Use your database service name props.put(\"oracle.service.name\", \"freepdb1\"); // Choose PLAINTEXT or SSL as appropriate for your database connection props.put(\"security.protocol\", \"SSL\"); // Your database server props.put(\"bootstrap.servers\", \"my-db-server\"); // Path to directory containing ojdbc.properties // If using Oracle Wallet, this directory must contain the unzipped wallet (such as in sqlnet.ora) props.put(\"oracle.net.tns_admin\", \"/my/path/\"); props.put(\"group.id\" , \"MY_CONSUMER_GROUP\"); // Set auto-commit to false for direct transaction management. props.put(\"enable.auto.commit\",\"false\"); props.put(\"max.poll.records\", 2000); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); KafkaConsumer\u003cString, String\u003e consumer = new KafkaConsumer\u003c\u003e(props); Consumer Methods To retrieve the current database connection within the transaction, use the consumer.getDBConnection() method. To commit the current transaction synchronously, use the consumer.commitSync() method. To commit the current transaction asynchronously, use the consumer.commitAsync() method. Transactional Consume Example The following Java method demonstrates how to use a KafkaConsumer for transactional messaging. Assume the processRecord method does some DML operation with the record, like inserting or updating a table.\npublic void run() { this.consumer.subscribe(List.of(\"topic1\")); while (true) { try { // 1. Poll a batch of records from the subscribed topics ConsumerRecords\u003cString, String\u003e records = consumer.poll( Duration.ofMillis(100) ); System.out.println(\"Consumed records: \" + records.count()); // 2. Get the current transaction's database connection Connection conn = consumer.getDBConnection(); for (ConsumerRecord\u003cString, String\u003e record : records) { // 3. Do some DML with the record and connection processRecord(record, conn); } // 4. Do a blocking commit on the current batch of records. For non-blocking, use commitAsync() consumer.commitSync(); } catch (Exception e) { // 5. Since auto-commit is disabled, transactions are not // committed when commitSync() is not called. System.out.println(\"Unexpected error processing records. Aborting transaction!\"); // Rollback DML from (3) consumer.getDBConnection().rollback(); } } }",
    "description": "This section provides a detailed walkthrough of transactional messaging using the Kafka Java Client for Oracle Database Transactional Event Queues. You’ll learn how to run, commit, and abort database transactions while using Kafka producers and consumers for transactional messaging.\nKafka Example The KafkaProducer and KafkaConsumer classes implemented by the Kafka Java Client for Oracle Transactional Event Queues provide functionality for transactional messaging, allowing developers to run database queries within a produce or consume transaction.",
    "tags": [],
    "title": "Transactional Messaging",
    "uri": "/microservices-datadriven/transactional-event-queues/kafka/transactional-messaging/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "Oracle Database 23ai includes a migration path from Advanced Queuing (AQ) to Transactional Event Queues (TxEventQ), to take advantage of enhanced performance and scalability for event-driven architectures.\nAdvanced Queuing (AQ) has been Oracle’s database messaging system for managing asynchronous communication in enterprise applications, allowing reliable queuing and message delivery. TxEventQ leverages Kafka-based event queuing, offering improved throughput, lower latency, and greater scalability, making it ideal for modern event-driven architectures and high-volume event processing.\nThe DBMS_AQMIGTOOL package facilitates a smooth migration process, designed to be non-disruptive and allowing the parallel operation of AQ and TxEventQ during the transition, enabling a smooth cut-over with minimal downtime for your applications.\nThe migration from AQ to TxEventQ is suitable for various scenarios:\nScaling up existing AQ-based applications Modernizing legacy messaging systems Improving performance for high-volume event processing",
    "description": "Oracle Database 23ai includes a migration path from Advanced Queuing (AQ) to Transactional Event Queues (TxEventQ), to take advantage of enhanced performance and scalability for event-driven architectures.\nAdvanced Queuing (AQ) has been Oracle’s database messaging system for managing asynchronous communication in enterprise applications, allowing reliable queuing and message delivery. TxEventQ leverages Kafka-based event queuing, offering improved throughput, lower latency, and greater scalability, making it ideal for modern event-driven architectures and high-volume event processing.",
    "tags": [],
    "title": "Migrating From AQ",
    "uri": "/microservices-datadriven/transactional-event-queues/aq-migration/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/microservices-datadriven/transactional-event-queues/categories/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues \u003e ",
    "content": "Contributors Thank you to the following contributors (listed in alphabetical order):\nAnders Swanson, Developer Evangelist, Oracle Database",
    "description": "Contributors Thank you to the following contributors (listed in alphabetical order):\nAnders Swanson, Developer Evangelist, Oracle Database",
    "tags": [],
    "title": "Credits",
    "uri": "/microservices-datadriven/transactional-event-queues/more/credits/index.html"
  },
  {
    "breadcrumb": "Transactional Event Queues",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/microservices-datadriven/transactional-event-queues/tags/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Oracle Transactional Event Queues (TxEventQ) is a messaging platform built into Oracle Database that combines the best features of messaging and pub/sub systems. TxEventQ was introduced as a rebranding of AQ Sharded Queues in Oracle Database 21c, evolving from the Advanced Queuing (AQ) technology that has been part of Oracle Database since version 8.0. TxEventQ continues to evolve in Oracle Database 23ai, with Kafka Java APIs, Oracle REST Data Services (ORDS) integration, and many more features and integrations.\nTxEventQ is designed for high-throughput, reliable messaging in event-driven microservices and workflow applications. It supports multiple publishers and consumers, exactly-once message delivery, and robust event streaming capabilities. On an 8-node Oracle Real Application Clusters (RAC) database, TxEventQ can handle approximately 1 million messages per second, demonstrating its scalability.\nTxEventQ differs from traditional AQ (now referred to as AQ Classic Queues) in several ways:\nPerformance: TxEventQ is designed for higher throughput and scalability.\nArchitecture: TxEventQ uses a partitioned implementation with multiple event streams per queue, while AQ is a disk-based implementation for simpler workflow use cases.\nEnhanced interoperability with Apache Kafka, JMS, Spring Boot, ORDS, and other software systems.\nKey features of Oracle Database Transactional Event Queues include:\nTransactional messaging: Enqueues and dequeues are automatically committed along with other database operations, eliminating the need for two-phase commits.\nExactly once message delivery.\nStrict ordering of messages within queue partitions.\nRetention of messages for a specified time after consumption by subscribers.\nCapability to query message queues and their metadata by standard SQL.\nTransactional outbox support: This simplifies event-driven application development for microservices.\nFor developers, TxEventQ can be integrated into modern application development environments using Oracle Database. It’s particularly useful in microservices architectures and event-driven applications where high-throughput, reliable messaging is crucial. The Kafka-compatible Java APIs allow developers to use existing Kafka code with minimal changes, simply by updating the broker address and using Oracle-specific versions of KafkaProducer and KafkaConsumer.\nOracle Database Transactional Event Queues are free to use with Oracle Database in any deployment, including Oracle Database Free.\nCitations:\nOracle Database Transactional Event Queues homepage Transactional Event Queue Documentation for Oracle Database 23ai Kafka Java Client for Oracle Database Transactional Event Queues",
    "description": "Oracle Transactional Event Queues (TxEventQ) is a messaging platform built into Oracle Database that combines the best features of messaging and pub/sub systems. TxEventQ was introduced as a rebranding of AQ Sharded Queues in Oracle Database 21c, evolving from the Advanced Queuing (AQ) technology that has been part of Oracle Database since version 8.0. TxEventQ continues to evolve in Oracle Database 23ai, with Kafka Java APIs, Oracle REST Data Services (ORDS) integration, and many more features and integrations.",
    "tags": [],
    "title": "Transactional Event Queues",
    "uri": "/microservices-datadriven/transactional-event-queues/index.html"
  }
]
