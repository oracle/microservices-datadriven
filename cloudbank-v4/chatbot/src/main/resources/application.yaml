spring:
  application:
    name: chatbot
  threads:
    virtual:
      enabled: true
  ai:
    ollama:
      base-url: http://ollama.ollama.svc.cluster.local:11434
      chat:
        enabled: true
        options:
          model: llama3

eureka:
  instance:
    hostname: ${spring.application.name}
    preferIpAddress: true
  client:
    service-url:
      defaultZone: ${eureka.service-url}
    fetch-registry: true
    register-with-eureka: true
    enabled: true

management:
  endpoint:
    health:
      show-details: always
      show-components: always
  endpoints:
    web:
      exposure:
        include: "*"
  metrics:
    tags:
      application: ${spring.application.name}
    distribution:
      percentiles[http.server.requests]: 0.5, 0.90, 0.95, 0.99
      percentiles-histogram[http.server.requests]: true
      slo[http.server.requests]: 100ms, 250ms, 500ms, 1s, 2s, 5s, 10s, 30s
      percentiles[http.client.requests]: 0.5, 0.90, 0.95, 0.99
      percentiles-histogram[http.client.requests]: true
      slo[http.client.requests]: 100ms, 250ms, 500ms, 1s, 2s, 5s, 10s, 30s
  health:
    probes:
      enabled: true
  tracing:
    sampling:
      probability: 1.0
  info:
    os:
      enabled: true
    env:
      enabled: true
    java:
      enabled: true
  otlp:
    tracing:
      endpoint: ${otel.exporter.otlp.endpoint}
  observations:
    key-values:
      app: ${spring.application.name}

logging:
  level:
    root: INFO
    com.example: DEBUG
